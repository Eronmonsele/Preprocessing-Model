{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f236bc9-1ba9-4e62-98b0-b2c66e63210e",
   "metadata": {},
   "source": [
    "Jeff_HPMS_FACTORS_AB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8bb3c-a3fd-40bb-aa96-b75762297fe8",
   "metadata": {},
   "source": [
    "# Moves Preprocessing Model for 2025, 2030, 2035, 2040 & 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22732f91-da32-4928-935d-83fe2fac1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook  # Import this for loading the Excel workbook\n",
    "from openpyxl.styles import Font\n",
    "# Import all these files\n",
    "TRANSCAD_RESULT_FILE = pd.read_csv('2025Transcad_file.csv')\n",
    "TRANSCAD_RESULT_FILE_2030 = pd.read_csv('2030Transcad_file.csv')\n",
    "TRANSCAD_RESULT_FILE_2035 = pd.read_csv('2035Transcad_file.csv')\n",
    "TRANSCAD_RESULT_FILE_2040 = pd.read_csv('2040Transcad_file.csv')\n",
    "TRANSCAD_RESULT_FILE_2050 = pd.read_csv('2050Transcad_file.csv')\n",
    "ATR_HOURLY_VOLUME = pd.read_csv('VOLPCT02_TT.csv')\n",
    "HPMS_FACTORS = pd.read_csv('HpmO3.csv')\n",
    "HER_FACTORS = pd.read_csv('HerO3.csv')\n",
    "Functional_classification = pd.read_csv('Functional_classification.csv')\n",
    "\n",
    "# Select relevant variables\n",
    "TRANSCAD_FILE = TRANSCAD_RESULT_FILE.loc[:, ['ID', 'Length', 'Link_AADT', \n",
    "                                             'County', 'AB_FACT', 'FUNCL', 'BA_FACT', \n",
    "                                              'AB_FFSPD', 'BA_FFSPD', 'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                              'BA_VOL_Fin', 'Tot_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "\n",
    "# Fill null values in Link_AADT and convert types\n",
    "TRANSCAD_FILE['Link_AADT'] = TRANSCAD_FILE['Link_AADT'].fillna(0)\n",
    "TRANSCAD_FILE[\"Link_AADT\"] = TRANSCAD_FILE[\"Link_AADT\"].astype(int)\n",
    "TRANSCAD_FILE[\"AB_CAPDAY\"] = TRANSCAD_FILE[\"AB_CAPDAY\"].astype(int)\n",
    "TRANSCAD_FILE[\"BA_CAPDAY\"] = TRANSCAD_FILE[\"BA_CAPDAY\"].astype(int)\n",
    "TRANSCAD_FILE[\"FUNCL\"] = TRANSCAD_FILE[\"FUNCL\"].astype(int)\n",
    "TRANSCAD_FILE[\"AB_FFSPD\"] = TRANSCAD_FILE[\"AB_FFSPD\"].astype(int)\n",
    "TRANSCAD_FILE[\"BA_FFSPD\"] = TRANSCAD_FILE[\"BA_FFSPD\"].astype(int)\n",
    "TRANSCAD_FILE[\"B_exponent\"] = TRANSCAD_FILE[\"B_exponent\"].astype(int)\n",
    "TRANSCAD_FILE[\"A_coefficient\"] = TRANSCAD_FILE[\"A_coefficient\"].astype(float)\n",
    "TRANSCAD_FILE[\"BA_VOL_Fin\"] = TRANSCAD_FILE[\"BA_VOL_Fin\"].astype(float)\n",
    "TRANSCAD_FILE[\"Length\"] = TRANSCAD_FILE[\"Length\"].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Peak Hour Capacity\n",
    "TRANSCAD_FILE['AB_Peak_HR_Capacity'] = TRANSCAD_FILE.AB_CAPDAY / 10\n",
    "TRANSCAD_FILE['BA_Peak_HR_Capacity'] = TRANSCAD_FILE.BA_CAPDAY / 10\n",
    "\n",
    "# Create duplicate points column\n",
    "TRANSCAD_FILE['AB_FUNCL'] = TRANSCAD_FILE.loc[:, 'FUNCL']\n",
    "TRANSCAD_FILE['BA_FUNCL'] = TRANSCAD_FILE.loc[:, 'FUNCL']\n",
    "TRANSCAD_FILE['AB_FUNCL_ATR'] = TRANSCAD_FILE.loc[:, 'FUNCL']\n",
    "TRANSCAD_FILE['BA_FUNCL_ATR'] = TRANSCAD_FILE.loc[:, 'FUNCL']\n",
    "\n",
    "#Library original\n",
    "TRANSCAD_FILE[\"AB_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                   8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                   18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "#Library\n",
    "TRANSCAD_FILE[\"BA_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                  8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                  18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "\n",
    "TRANSCAD_FILE[\"AB_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "TRANSCAD_FILE[\"BA_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffcc6f42-25ee-423b-b8cd-1088cf6f97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating different county files \n",
    "Bullit_TRANSCAD_FILE=TRANSCAD_FILE.loc[TRANSCAD_FILE.County=='Bullitt', :]\n",
    "\n",
    "Clark_TRANSCAD_FILE=TRANSCAD_FILE.loc[TRANSCAD_FILE.County=='Clark', :]\n",
    "\n",
    "Floyd_TRANSCAD_FILE=TRANSCAD_FILE.loc[TRANSCAD_FILE.County=='Floyd', :]\n",
    "\n",
    "Jefferson_TRANSCAD_FILE=TRANSCAD_FILE.loc[TRANSCAD_FILE.County=='Jefferson', :]\n",
    "\n",
    "Oldham_TRANSCAD_FILE=TRANSCAD_FILE.loc[TRANSCAD_FILE.County=='Oldham', :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9561155-0cc1-48cf-8717-2e7ddd98e94b",
   "metadata": {},
   "source": [
    "# 2025 Bullit County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57faccb7-783a-4048-9ff4-4be16b64f68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\29625885.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "#Bullit \n",
    "# Selecting needed files in Bullit AB\n",
    "Bullit_TRANSCAD_FILE_AB = Bullit_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_AB = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_AB = Bullit_HPMS_FACTORS_AB.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_AB['AB_FUNCL'] = Bullit_TRANSCAD_FILE_AB['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_AB['AB_FUNCL'] = Bullit_HPMS_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB['AB_FUNCL'] = HER_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume = Bullit_TRANSCAD_FILE_AB.merge(Bullit_HPMS_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "Bullit_July_volume['AB_July_Volume'] = Bullit_July_volume.AB_VOL_Fin * Bullit_July_volume.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume = Bullit_July_volume.merge(ATR_HOURLY_VOLUME_AB, on='AB_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume = Bullit_July_volume.merge(HER_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only = Bullit_July_volume.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix = Bullit_July_volume.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix = Bullit_July_24hr_matrix[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix[\"AB_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix = Bullit_July_24hr_matrix.fillna(0)\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume = pd.concat([Bullit_July_volume_Only,Bullit_July_24hr_matrix ], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume)\n",
    "\n",
    "\n",
    "#2\n",
    "# Selecting needed files in Bullit BA\n",
    "Bullit_TRANSCAD_FILE_BA = Bullit_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_BA = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_BA = Bullit_HPMS_FACTORS_BA.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_BA['BA_FUNCL'] = Bullit_TRANSCAD_FILE_BA['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_BA['BA_FUNCL'] = Bullit_HPMS_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA['BA_FUNCL'] = HER_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "\n",
    "\n",
    "# Create the adjusted volume of for the month of July using the HPMS file. \n",
    "Bullit_July_volume_BA = Bullit_TRANSCAD_FILE_BA.merge(Bullit_HPMS_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "Bullit_July_volume_BA['BA_July_Volume'] = Bullit_July_volume_BA.BA_VOL_Fin * Bullit_July_volume_BA.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_BA = Bullit_July_volume_BA.merge(ATR_HOURLY_VOLUME_BA, on='BA_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_BA = Bullit_July_volume_BA.merge(HER_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_BA = Bullit_July_volume_BA.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\",'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_BA = Bullit_July_volume_BA.loc[:, [\"ID\", \"BA_FUNCL\",'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_BA = Bullit_July_24hr_matrix_BA[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_BA[\"BA_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_BA = Bullit_July_24hr_matrix_BA.fillna(0)\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_BA = pd.concat([Bullit_July_volume_Only_BA,Bullit_July_24hr_matrix_BA ], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for Bullitt County\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_BA)\n",
    "\n",
    "# Bullitt_July_Hourly_Volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Call the function for AB direction in Bullit\n",
    "funcl_summary_Bullit_ab = sum_new_vmt_vht_and_speed_by_direction(Bullit_July_Hourly_Volume, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Bullit\n",
    "funcl_summary_Bullit_ba = sum_new_vmt_vht_and_speed_by_direction(Bullit_July_Hourly_Volume_BA, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Bullit_ab = funcl_summary_Bullit_ab.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ab = funcl_summary_Bullit_ab.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Bullit_ba = funcl_summary_Bullit_ba.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ba = funcl_summary_Bullit_ba.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary = pd.merge(funcl_summary_Bullit_ab, funcl_summary_Bullit_ba, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary['Total_NEW_VMT'] = combined_summary['Total_NEW_VMT_AB'].fillna(0) + combined_summary['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary['Total_VHT'] = combined_summary['Total_VHT_AB'].fillna(0) + combined_summary['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab = combined_summary['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba = combined_summary['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary['Mean_Congested_Speed'] = (mean_speed_ab + mean_speed_ba) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Bullit\n",
    "Bullit_VMT = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Bullit\n",
    "#print(Bullit_VMT)\n",
    "#Bullit_VMT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming Bullit_VMT is defined elsewhere in your code\n",
    "Bullit_VMT_df = pd.DataFrame(Bullit_VMT)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\",\n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Bullit_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Bullit_VMT_df = pd.merge(expected_Bullit_VMT_df, Bullit_VMT_df, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Bullit_VMT_df[\"Total_NEW_VMT\"] = final_Bullit_VMT_df[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Bullit_VMT_df[\"Total_VHT\"] = final_Bullit_VMT_df[\"Total_VHT\"].astype(float)\n",
    "final_Bullit_VMT_df[\"Mean_Congested_Speed\"] = final_Bullit_VMT_df[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Bullit_VMT_df[final_Bullit_VMT_df['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Bullit_VMT_df[final_Bullit_VMT_df['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Bullit_VMT_df = pd.concat([final_Bullit_VMT_df, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Bullit_VMT_df['Speed'] = final_Bullit_VMT_df['Total_NEW_VMT'] / final_Bullit_VMT_df['Total_VHT']\n",
    "\n",
    "\n",
    "# Ramps!! 5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Bullit_VMT_df) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "#print(final_Bullit_VMT_df)\n",
    "#final_Bullit_VMT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1eb9bc5-9dbc-43ff-b39c-02f166e5bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame with VMT shares\n",
    "#final_Bullit_VMT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02c024-1033-4af9-b7cc-a31c9d7448cc",
   "metadata": {},
   "source": [
    "# 2025 Clark County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7531b9-03d8-463e-af86-3b701faf2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1808258351.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Clark\n",
    "# Selecting needed files in Clark AB\n",
    "Clark_TRANSCAD_FILE_AB = Clark_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_AB = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_AB = Clark_HPMS_FACTORS_AB.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_AB['AB_FUNCL'] = Clark_TRANSCAD_FILE_AB['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_AB['AB_FUNCL'] = Clark_HPMS_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB['AB_FUNCL'] = HER_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume = Clark_TRANSCAD_FILE_AB.merge(Clark_HPMS_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "Clark_July_volume['AB_July_Volume'] = Clark_July_volume.AB_VOL_Fin * Clark_July_volume.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume = Clark_July_volume.merge(ATR_HOURLY_VOLUME_AB, on='AB_FUNCL_ATR', how='left')\n",
    "Clark_July_volume = Clark_July_volume.merge(HER_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only = Clark_July_volume.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix = Clark_July_volume.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix = Clark_July_24hr_matrix[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix[\"AB_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix = Clark_July_24hr_matrix.fillna(0)\n",
    "\n",
    "\n",
    "Clark_July_Hourly_Volume = pd.concat([Clark_July_volume_Only, Clark_July_24hr_matrix], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume)\n",
    "\n",
    "\n",
    "# Selecting needed files in Clark BA\n",
    "Clark_TRANSCAD_FILE_BA = Clark_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_BA = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_BA = Clark_HPMS_FACTORS_BA.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_BA['BA_FUNCL'] = Clark_TRANSCAD_FILE_BA['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_BA['BA_FUNCL'] = Clark_HPMS_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA['BA_FUNCL'] = HER_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_BA = Clark_TRANSCAD_FILE_BA.merge(Clark_HPMS_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "Clark_July_volume_BA['BA_July_Volume'] = Clark_July_volume_BA.BA_VOL_Fin * Clark_July_volume_BA.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_BA = Clark_July_volume_BA.merge(ATR_HOURLY_VOLUME_BA, on='BA_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_BA = Clark_July_volume_BA.merge(HER_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_BA = Clark_July_volume_BA.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\",'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_BA = Clark_July_volume_BA.loc[:, [\"ID\", \"BA_FUNCL\",'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_BA = Clark_July_24hr_matrix_BA[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_BA[\"BA_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_BA = Clark_July_24hr_matrix_BA.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_BA = pd.concat([Clark_July_volume_Only_BA, Clark_July_24hr_matrix_BA], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for Clark County\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_BA)\n",
    "\n",
    "# Clark_July_Hourly_Volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Call the function for AB direction in Clark\n",
    "funcl_summary_Clark_ab = sum_new_vmt_vht_and_speed_by_direction(Clark_July_Hourly_Volume, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Clark\n",
    "funcl_summary_Clark_ba = sum_new_vmt_vht_and_speed_by_direction(Clark_July_Hourly_Volume_BA, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Clark_ab = funcl_summary_Clark_ab.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ab = funcl_summary_Clark_ab.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Clark_ba = funcl_summary_Clark_ba.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ba = funcl_summary_Clark_ba.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary = pd.merge(funcl_summary_Clark_ab, funcl_summary_Clark_ba, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary['Total_NEW_VMT'] = combined_summary['Total_NEW_VMT_AB'].fillna(0) + combined_summary['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary['Total_VHT'] = combined_summary['Total_VHT_AB'].fillna(0) + combined_summary['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab = combined_summary['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba = combined_summary['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary['Mean_Congested_Speed'] = (mean_speed_ab + mean_speed_ba) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Clark\n",
    "Clark_VMT = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Clark\n",
    "#print(Clark_VMT)\n",
    "#Clark_VMT\n",
    "\n",
    "\n",
    "# Assuming Clark_VMT is defined elsewhere in your code\n",
    "Clark_VMT_df = pd.DataFrame(Clark_VMT)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\",\n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\",\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Clark_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Clark_VMT_df = pd.merge(expected_Clark_VMT_df, Clark_VMT_df, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Clark_VMT_df[\"Total_NEW_VMT\"] = final_Clark_VMT_df[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Clark_VMT_df[\"Total_VHT\"] = final_Clark_VMT_df[\"Total_VHT\"].astype(float)\n",
    "final_Clark_VMT_df[\"Mean_Congested_Speed\"] = final_Clark_VMT_df[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "final_Clark_VMT_df['Speed'] = final_Clark_VMT_df['Total_NEW_VMT'] / final_Clark_VMT_df['Total_VHT']\n",
    "#final_Clark_VMT_df\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Clark_VMT_df[final_Clark_VMT_df['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Clark_VMT_df[final_Clark_VMT_df['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Clark_VMT_df = pd.concat([final_Clark_VMT_df, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Clark_VMT_df['Speed'] = final_Clark_VMT_df['Total_NEW_VMT'] / final_Clark_VMT_df['Total_VHT']\n",
    "\n",
    "\n",
    "# Ramps!! 5\n",
    "\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Clark_VMT_df) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Clark_VMT_df = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Clark_VMT_df = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Clark_VMT_df = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Clark_VMT_df = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame with VMT shares\n",
    "#print(final_Clark_VMT_df)\n",
    "#final_Clark_VMT_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ccb22ce-0347-423c-b8f8-aa5f397c23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Clark_VMT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8c344-9532-4b6c-b79a-b49732938f9a",
   "metadata": {},
   "source": [
    "# 2025 Floyd County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "933a3a48-ae2a-4565-a79d-74ff93298b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:192: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:192: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:339: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] +\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:351: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1391146215.py:352: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n"
     ]
    }
   ],
   "source": [
    "# Floyd\n",
    "# Selecting needed files in Floyd AB\n",
    "Floyd_TRANSCAD_FILE_AB = Floyd_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_AB = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_AB = Floyd_HPMS_FACTORS_AB.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_AB['AB_FUNCL'] = Floyd_TRANSCAD_FILE_AB['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_AB['AB_FUNCL'] = Floyd_HPMS_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB['AB_FUNCL'] = HER_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume = Floyd_TRANSCAD_FILE_AB.merge(Floyd_HPMS_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "Floyd_July_volume['AB_July_Volume'] = Floyd_July_volume.AB_VOL_Fin * Floyd_July_volume.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume = Floyd_July_volume.merge(ATR_HOURLY_VOLUME_AB, on='AB_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume = Floyd_July_volume.merge(HER_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only = Floyd_July_volume.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix = Floyd_July_volume.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix = Floyd_July_24hr_matrix[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix[\"AB_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix = Floyd_July_24hr_matrix.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume = pd.concat([Floyd_July_volume_Only, Floyd_July_24hr_matrix], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume)\n",
    "\n",
    "# Selecting needed files in Floyd BA\n",
    "Floyd_TRANSCAD_FILE_BA = Floyd_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_BA = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_BA = Floyd_HPMS_FACTORS_BA.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_BA['BA_FUNCL'] = Floyd_TRANSCAD_FILE_BA['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_BA['BA_FUNCL'] = Floyd_HPMS_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA['BA_FUNCL'] = HER_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_BA = Floyd_TRANSCAD_FILE_BA.merge(Floyd_HPMS_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "Floyd_July_volume_BA['BA_July_Volume'] = Floyd_July_volume_BA.BA_VOL_Fin * Floyd_July_volume_BA.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_BA = Floyd_July_volume_BA.merge(ATR_HOURLY_VOLUME_BA, on='BA_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_BA = Floyd_July_volume_BA.merge(HER_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_BA = Floyd_July_volume_BA.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_BA = Floyd_July_volume_BA.loc[:, [\"ID\", \"BA_FUNCL\", 'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_BA = Floyd_July_24hr_matrix_BA[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_BA[\"BA_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_BA = Floyd_July_24hr_matrix_BA.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_BA = pd.concat([Floyd_July_volume_Only_BA, Floyd_July_24hr_matrix_BA], axis=1, join='inner')\n",
    "\n",
    "# Apply the BPR calculations for BA\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for Floyd County\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_BA)\n",
    "\n",
    "# Floyd_July_Hourly_Volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Call the function for AB direction in Floyd\n",
    "funcl_summary_Floyd_ab = sum_new_vmt_vht_and_speed_by_direction(Floyd_July_Hourly_Volume, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Floyd\n",
    "funcl_summary_Floyd_ba = sum_new_vmt_vht_and_speed_by_direction(Floyd_July_Hourly_Volume_BA, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Floyd_ab = funcl_summary_Floyd_ab.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ab = funcl_summary_Floyd_ab.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Floyd_ba = funcl_summary_Floyd_ba.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ba = funcl_summary_Floyd_ba.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary = pd.merge(funcl_summary_Floyd_ab, funcl_summary_Floyd_ba, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary['Total_NEW_VMT'] = combined_summary['Total_NEW_VMT_AB'].fillna(0) + combined_summary['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary['Total_VHT'] = combined_summary['Total_VHT_AB'].fillna(0) + combined_summary['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab = combined_summary['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba = combined_summary['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary['Mean_Congested_Speed'] = (mean_speed_ab + mean_speed_ba) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Floyd\n",
    "Floyd_VMT = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Floyd\n",
    "#print(Floyd_VMT)\n",
    "#Floyd_VMT\n",
    "\n",
    "\n",
    "\n",
    "# Assuming Floyd_VMT is defined elsewhere in your code\n",
    "Floyd_VMT_df = pd.DataFrame(Floyd_VMT)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\", \n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Floyd_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Floyd_VMT_df = pd.merge(expected_Floyd_VMT_df, Floyd_VMT_df, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Floyd_VMT_df[\"Total_NEW_VMT\"] = final_Floyd_VMT_df[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Floyd_VMT_df[\"Total_VHT\"] = final_Floyd_VMT_df[\"Total_VHT\"].astype(float)\n",
    "final_Floyd_VMT_df[\"Mean_Congested_Speed\"] = final_Floyd_VMT_df[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Floyd_VMT_df[final_Floyd_VMT_df['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Floyd_VMT_df[final_Floyd_VMT_df['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Floyd_VMT_df = pd.concat([final_Floyd_VMT_df, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Floyd_VMT_df['Speed'] = final_Floyd_VMT_df['Total_NEW_VMT'] / final_Floyd_VMT_df['Total_VHT']\n",
    "\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Floyd_VMT_df) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Floyd_VMT_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4a2cbc-a804-4816-b81e-39ea26e249b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Floyd_VMT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aac643-3454-4f63-a585-6234802a878b",
   "metadata": {},
   "source": [
    "# 2025 Jefferson County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcea8d3-2b47-4bb0-94a8-2b766fcebb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:196: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1510151610.py:196: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Selecting needed files in Jefferson AB\n",
    "Jefferson_TRANSCAD_FILE_AB = Jefferson_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', 'BA_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jeff_HPMS_FACTORS_AB = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jeff_HPMS_FACTORS_AB = Jeff_HPMS_FACTORS_AB.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_AB['AB_FUNCL'] = Jefferson_TRANSCAD_FILE_AB['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'].astype(str)\n",
    "Jeff_HPMS_FACTORS_AB['AB_FUNCL'] = Jeff_HPMS_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB['AB_FUNCL'] = HER_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume of for the month of July using the HPMS file. \n",
    "Jefferson_July_volume = Jefferson_TRANSCAD_FILE_AB.merge(Jeff_HPMS_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "Jefferson_July_volume['AB_July_Volume'] = Jefferson_July_volume.AB_VOL_Fin * Jefferson_July_volume.JEFF\n",
    "\n",
    "# Create the adjusted volume of for the month of July using the HPMS file. \n",
    "#Jefferson_July_volume = Jefferson_TRANSCAD_FILE.merge(Jeff_HPMS_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "#Jefferson_July_volume['AB_July_Volume'] = Jefferson_July_volume.AB_VOL_Fin * Jefferson_July_volume.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume = Jefferson_July_volume.merge(ATR_HOURLY_VOLUME_AB, on='AB_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume = Jefferson_July_volume.merge(HER_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only = Jefferson_July_volume.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix = Jefferson_July_volume.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix = Jefferson_July_24hr_matrix[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix[\"AB_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix = Jefferson_July_24hr_matrix.fillna(0)\n",
    "\n",
    "\n",
    "Jefferson_July_Hourly_Volume = pd.concat([Jefferson_July_volume_Only,Jefferson_July_24hr_matrix ], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume)\n",
    "\n",
    "#Jefferson_July_Hourly_Volume\n",
    "\n",
    "# Selecting needed files in Jefferson BA\n",
    "Jefferson_TRANSCAD_FILE_BA = Jefferson_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jeff_HPMS_FACTORS_BA = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jeff_HPMS_FACTORS_BA = Jeff_HPMS_FACTORS_BA.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_BA['BA_FUNCL'] = Jefferson_TRANSCAD_FILE_BA['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'].astype(str)\n",
    "Jeff_HPMS_FACTORS_BA['BA_FUNCL'] = Jeff_HPMS_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA['BA_FUNCL'] = HER_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "\n",
    "\n",
    "# Create the adjusted volume of for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_BA = Jefferson_TRANSCAD_FILE_BA.merge(Jeff_HPMS_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "Jefferson_July_volume_BA['BA_July_Volume'] = Jefferson_July_volume_BA.BA_VOL_Fin * Jefferson_July_volume_BA.JEFF\n",
    "#Jefferson_July_volume_BA\n",
    "\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_BA = Jefferson_July_volume_BA.merge(ATR_HOURLY_VOLUME_BA, on='BA_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_BA = Jefferson_July_volume_BA.merge(HER_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_BA = Jefferson_July_volume_BA.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\",'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_BA = Jefferson_July_volume_BA.loc[:, [\"ID\", \"BA_FUNCL\",'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_BA = Jefferson_July_24hr_matrix_BA[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_BA[\"BA_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_BA = Jefferson_July_24hr_matrix_BA.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_BA = pd.concat([Jefferson_July_volume_Only_BA, Jefferson_July_24hr_matrix_BA], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_BA)\n",
    "\n",
    "# Jefferson_July_Hourly_Volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Call the function for AB direction in Jefferson\n",
    "funcl_summary_Jefferson_ab = sum_new_vmt_vht_and_speed_by_direction(Jefferson_July_Hourly_Volume, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Jefferson\n",
    "funcl_summary_Jefferson_ba = sum_new_vmt_vht_and_speed_by_direction(Jefferson_July_Hourly_Volume_BA, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Jefferson_ab = funcl_summary_Jefferson_ab.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ab = funcl_summary_Jefferson_ab.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Jefferson_ba = funcl_summary_Jefferson_ba.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ba = funcl_summary_Jefferson_ba.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary = pd.merge(funcl_summary_Jefferson_ab, funcl_summary_Jefferson_ba, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary['Total_NEW_VMT'] = combined_summary['Total_NEW_VMT_AB'].fillna(0) + combined_summary['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary['Total_VHT'] = combined_summary['Total_VHT_AB'].fillna(0) + combined_summary['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab = combined_summary['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba = combined_summary['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary['Mean_Congested_Speed'] = (mean_speed_ab + mean_speed_ba) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Jefferson\n",
    "Jefferson_VMT = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Jefferson\n",
    "#print(Jefferson_VMT)\n",
    "#Jefferson_VMT\n",
    "\n",
    "\n",
    "\n",
    "# Assuming Jefferson_VMT is defined elsewhere in your code\n",
    "Jefferson_VMT_df = pd.DataFrame(Jefferson_VMT)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \"NOT USED\",\n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\", \"NOT USED\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Jefferson_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Jefferson_VMT_df = pd.merge(expected_Jefferson_VMT_df, Jefferson_VMT_df, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Jefferson_VMT_df[\"Total_NEW_VMT\"] = final_Jefferson_VMT_df[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Jefferson_VMT_df[\"Total_VHT\"] = final_Jefferson_VMT_df[\"Total_VHT\"].astype(float)\n",
    "final_Jefferson_VMT_df[\"Mean_Congested_Speed\"] = final_Jefferson_VMT_df[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "#4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Jefferson_VMT_df[final_Jefferson_VMT_df['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Jefferson_VMT_df[final_Jefferson_VMT_df['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Jefferson_VMT_df = pd.concat([final_Jefferson_VMT_df, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Jefferson_VMT_df['Speed'] = final_Jefferson_VMT_df['Total_NEW_VMT'] / final_Jefferson_VMT_df['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Jefferson_VMT_df) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Jefferson_VMT_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e9c4df-5de0-47b6-811c-46d283c4e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Jefferson_VMT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b9586-bf3a-4fa6-857e-b57a3ffdf307",
   "metadata": {},
   "source": [
    "# 2025 Oldham County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b3540f8-baa2-4ccd-bcfd-c69f3cac339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3738503322.py:190: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Oldham\n",
    "# Selecting needed files in Oldham AB\n",
    "Oldham_TRANSCAD_FILE_AB = Oldham_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_AB = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_AB = Oldham_HPMS_FACTORS_AB.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_AB['AB_FUNCL'] = Oldham_TRANSCAD_FILE_AB['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB['AB_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_AB['AB_FUNCL'] = Oldham_HPMS_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB['AB_FUNCL'] = HER_FACTORS_AB['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume = Oldham_TRANSCAD_FILE_AB.merge(Oldham_HPMS_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "Oldham_July_volume['AB_July_Volume'] = Oldham_July_volume.AB_VOL_Fin * Oldham_July_volume.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume = Oldham_July_volume.merge(ATR_HOURLY_VOLUME_AB, on='AB_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume = Oldham_July_volume.merge(HER_FACTORS_AB, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only = Oldham_July_volume.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix = Oldham_July_volume.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix = Oldham_July_24hr_matrix[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix[\"AB_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix = Oldham_July_24hr_matrix.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume = pd.concat([Oldham_July_volume_Only, Oldham_July_24hr_matrix], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume)\n",
    "\n",
    "# Selecting needed files in Oldham BA\n",
    "Oldham_TRANSCAD_FILE_BA = Oldham_TRANSCAD_FILE.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_BA = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_BA = Oldham_HPMS_FACTORS_BA.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_BA['BA_FUNCL'] = Oldham_TRANSCAD_FILE_BA['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA['BA_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_BA['BA_FUNCL'] = Oldham_HPMS_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA['BA_FUNCL'] = HER_FACTORS_BA['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_BA = Oldham_TRANSCAD_FILE_BA.merge(Oldham_HPMS_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "Oldham_July_volume_BA['BA_July_Volume'] = Oldham_July_volume_BA.BA_VOL_Fin * Oldham_July_volume_BA.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_BA = Oldham_July_volume_BA.merge(ATR_HOURLY_VOLUME_BA, on='BA_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_BA = Oldham_July_volume_BA.merge(HER_FACTORS_BA, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_BA = Oldham_July_volume_BA.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\",'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_BA = Oldham_July_volume_BA.loc[:, [\"ID\", \"BA_FUNCL\",'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_BA = Oldham_July_24hr_matrix_BA[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_BA[\"BA_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_BA = Oldham_July_24hr_matrix_BA.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_BA = pd.concat([Oldham_July_volume_Only_BA, Oldham_July_24hr_matrix_BA], axis=1, join='inner')\n",
    "\n",
    "# Apply the BPR calculations for BA\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for Oldham County\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_BA)\n",
    "\n",
    "# Oldham_July_Hourly_Volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df = pd.concat([summary_df, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Call the function for AB direction in Oldham\n",
    "funcl_summary_Oldham_ab = sum_new_vmt_vht_and_speed_by_direction(Oldham_July_Hourly_Volume, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Oldham\n",
    "funcl_summary_Oldham_ba = sum_new_vmt_vht_and_speed_by_direction(Oldham_July_Hourly_Volume_BA, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Oldham_ab = funcl_summary_Oldham_ab.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ab = funcl_summary_Oldham_ab.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Oldham_ba = funcl_summary_Oldham_ba.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ba = funcl_summary_Oldham_ba.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary = pd.merge(funcl_summary_Oldham_ab, funcl_summary_Oldham_ba, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary['Total_NEW_VMT'] = combined_summary['Total_NEW_VMT_AB'].fillna(0) + combined_summary['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary['Total_VHT'] = combined_summary['Total_VHT_AB'].fillna(0) + combined_summary['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab = combined_summary['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba = combined_summary['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary['Mean_Congested_Speed'] = (mean_speed_ab + mean_speed_ba) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Oldham\n",
    "Oldham_VMT = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Oldham\n",
    "#print(Oldham_VMT)\n",
    "#Oldham_VMT\n",
    "\n",
    "\n",
    "Oldham_VMT_df = pd.DataFrame(Oldham_VMT)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \"NOT USED\",\n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\", \"NOT USED\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Oldham_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Oldham_VMT_df = pd.merge(expected_Oldham_VMT_df, Oldham_VMT_df, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Oldham_VMT_df[\"Total_NEW_VMT\"] = final_Oldham_VMT_df[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Oldham_VMT_df[\"Total_VHT\"] = final_Oldham_VMT_df[\"Total_VHT\"].astype(float)\n",
    "final_Oldham_VMT_df[\"Mean_Congested_Speed\"] = final_Oldham_VMT_df[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "#4 \n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Oldham_VMT_df[final_Oldham_VMT_df['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Oldham_VMT_df[final_Oldham_VMT_df['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Oldham_VMT_df = pd.concat([final_Oldham_VMT_df, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Oldham_VMT_df['Speed'] = final_Oldham_VMT_df['Total_NEW_VMT'] / final_Oldham_VMT_df['Total_VHT']\n",
    "\n",
    "\n",
    "#5 \n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Oldham_VMT_df) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Oldham_VMT_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "813b35fe-43d5-470a-826c-2616536da451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Oldham_VMT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486d544-2a34-4ff3-9664-a3368ded6ac6",
   "metadata": {},
   "source": [
    "# Combine all 2025 counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8126b5-9632-4a24-aa74-d5c6c8806a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Create a blank DataFrame with the same columns to use for spacing\n",
    "blank_lines = pd.DataFrame(np.nan, index=range(5), columns=Clark_VMT.columns)\n",
    "\n",
    "# Initialize a list to hold the DataFrames and their titles\n",
    "dataframes_with_titles = []\n",
    "\n",
    "# Append each DataFrame with its title and blank lines\n",
    "for county, df in zip(['Bullit','Clark', 'Floyd', 'Jefferson', 'Oldham'], \n",
    "                      [final_Bullit_VMT_df, final_Clark_VMT_df, final_Floyd_VMT_df, final_Jefferson_VMT_df, final_Oldham_VMT_df]):\n",
    "    # Create a DataFrame for the title (bold)\n",
    "    title_df = pd.DataFrame(columns=df.columns)\n",
    "    title_df.loc[0] = df.columns  # Set the title row\n",
    "    # Append the county name with \"County\" as the title\n",
    "    dataframes_with_titles.append((f\"{county} County\", title_df, df, blank_lines))\n",
    "\n",
    "# Create a new Excel workbook\n",
    "output_file_path = '2025_Combined_VMT_Data.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = 'VMT_Data'\n",
    "\n",
    "# Add the dataframes and format them\n",
    "row_offset = 0\n",
    "for title, title_df, df, blanks in dataframes_with_titles:\n",
    "    # Add the file name with \"County\" as a subheading above each DataFrame\n",
    "    ws.cell(row=row_offset + 1, column=1, value=title).font = Font(bold=True)\n",
    "    row_offset += 1  # Move to the next row for the title\n",
    "\n",
    "    # Write the column titles (bold)\n",
    "    for c_idx, column_name in enumerate(title_df.columns, start=1):\n",
    "        ws.cell(row=row_offset + 1, column=c_idx, value=column_name).font = Font(bold=True)\n",
    "\n",
    "    # Write the DataFrame data\n",
    "    for r_idx, row in enumerate(df.values, start=row_offset + 2):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Adjust column widths based on the maximum length of data in each column\n",
    "    for c_idx in range(1, df.shape[1] + 1):\n",
    "        max_length = 0\n",
    "        # Check title length\n",
    "        max_length = max(max_length, len(title_df.columns[c_idx - 1]))  \n",
    "        # Check data length\n",
    "        for r_idx in range(df.shape[0]):\n",
    "            cell_value = str(df.iat[r_idx, c_idx - 1])  # Get cell value\n",
    "            max_length = max(max_length, len(cell_value))  # Update max length if necessary\n",
    "        ws.column_dimensions[ws.cell(row=1, column=c_idx).column_letter].width = max_length + 2  # Add some padding\n",
    "\n",
    "    # Update the row_offset for the next DataFrame\n",
    "    row_offset += df.shape[0] + 3  # 2 for the data and 1 for the title\n",
    "\n",
    "    # Add blank lines\n",
    "    for _ in range(5):\n",
    "        row_offset += 1  # Add blank lines between DataFrames\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cccb4-4ebc-4ccf-9214-bacc55cfe96b",
   "metadata": {},
   "source": [
    "# 2030 Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed2fcb7a-99ab-4b0c-89ec-025332019d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant variables\n",
    "TRANSCAD_RESULT_FILE_2030 = TRANSCAD_RESULT_FILE_2030.loc[:, ['ID', 'Length', 'Link_AADT', \n",
    "                                             'County', 'AB_FACT', 'FUNCL', 'BA_FACT', \n",
    "                                              'AB_FFSPD', 'BA_FFSPD', 'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                              'BA_VOL_Fin', 'Tot_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "\n",
    "# Fill null values in Link_AADT and convert types\n",
    "TRANSCAD_RESULT_FILE_2030['Link_AADT'] = TRANSCAD_RESULT_FILE_2030['Link_AADT'].fillna(0)\n",
    "TRANSCAD_RESULT_FILE_2030[\"Link_AADT\"] = TRANSCAD_RESULT_FILE_2030[\"Link_AADT\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"AB_CAPDAY\"] = TRANSCAD_RESULT_FILE_2030[\"AB_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"BA_CAPDAY\"] = TRANSCAD_RESULT_FILE_2030[\"BA_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"FUNCL\"] = TRANSCAD_RESULT_FILE_2030[\"FUNCL\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"AB_FFSPD\"] = TRANSCAD_RESULT_FILE_2030[\"AB_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"BA_FFSPD\"] = TRANSCAD_RESULT_FILE_2030[\"BA_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"B_exponent\"] = TRANSCAD_RESULT_FILE_2030[\"B_exponent\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2030[\"A_coefficient\"] = TRANSCAD_RESULT_FILE_2030[\"A_coefficient\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2030[\"BA_VOL_Fin\"] = TRANSCAD_RESULT_FILE_2030[\"BA_VOL_Fin\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2030[\"Length\"] = TRANSCAD_RESULT_FILE_2030[\"Length\"].astype(float)\n",
    "\n",
    "\n",
    "# Calculate Peak Hour Capacity\n",
    "TRANSCAD_RESULT_FILE_2030['AB_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2030.AB_CAPDAY / 10\n",
    "TRANSCAD_RESULT_FILE_2030['BA_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2030.BA_CAPDAY / 10\n",
    "\n",
    "# Create duplicate points column\n",
    "TRANSCAD_RESULT_FILE_2030['AB_FUNCL'] = TRANSCAD_RESULT_FILE_2030.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2030['BA_FUNCL'] = TRANSCAD_RESULT_FILE_2030.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2030['AB_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2030.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2030['BA_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2030.loc[:, 'FUNCL']\n",
    "\n",
    "#Library original\n",
    "TRANSCAD_RESULT_FILE_2030[\"AB_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                   8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                   18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "#Library\n",
    "TRANSCAD_RESULT_FILE_2030[\"BA_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                  8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                  18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2030[\"AB_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2030[\"BA_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d535799-8eac-4ad2-b403-1ecf509d0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating different county files \n",
    "Bullit_TRANSCAD_FILE_2030=TRANSCAD_RESULT_FILE_2030.loc[TRANSCAD_RESULT_FILE_2030.County=='Bullitt', :]\n",
    "\n",
    "Clark_TRANSCAD_FILE_2030=TRANSCAD_RESULT_FILE_2030.loc[TRANSCAD_RESULT_FILE_2030.County=='Clark', :]\n",
    "\n",
    "Floyd_TRANSCAD_FILE_2030=TRANSCAD_RESULT_FILE_2030.loc[TRANSCAD_RESULT_FILE_2030.County=='Floyd', :]\n",
    "\n",
    "Jefferson_TRANSCAD_FILE_2030=TRANSCAD_RESULT_FILE_2030.loc[TRANSCAD_RESULT_FILE_2030.County=='Jefferson', :]\n",
    "\n",
    "Oldham_TRANSCAD_FILE_2030=TRANSCAD_RESULT_FILE_2030.loc[TRANSCAD_RESULT_FILE_2030.County=='Oldham', :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbae768-58a0-41ab-82d3-592c7089be86",
   "metadata": {},
   "source": [
    "# 2030 Bullit Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb3a7dc8-807a-4c1b-a414-c8c05986155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:198: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1974795711.py:198: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Bullit \n",
    "# Selecting needed files in Bullit AB\n",
    "Bullit_TRANSCAD_FILE_AB_2030 = Bullit_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_AB_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_AB_2030 = Bullit_HPMS_FACTORS_AB_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_AB_2030['AB_FUNCL'] = Bullit_TRANSCAD_FILE_AB_2030['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_AB_2030['AB_FUNCL'] = Bullit_HPMS_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2030['AB_FUNCL'] = HER_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume_2030 = Bullit_TRANSCAD_FILE_AB_2030.merge(Bullit_HPMS_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "Bullit_July_volume_2030['AB_July_Volume'] = Bullit_July_volume_2030.AB_VOL_Fin * Bullit_July_volume_2030.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_2030 = Bullit_July_volume_2030.merge(ATR_HOURLY_VOLUME_AB_2030, on='AB_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_2030 = Bullit_July_volume_2030.merge(HER_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_2030 = Bullit_July_volume_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_2030 = Bullit_July_volume_2030.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_2030 = Bullit_July_24hr_matrix_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_2030[\"AB_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_2030 = Bullit_July_24hr_matrix_2030.fillna(0)\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_2030 = pd.concat([Bullit_July_volume_Only_2030,Bullit_July_24hr_matrix_2030 ], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_2030)\n",
    "\n",
    "\n",
    "#2\n",
    "# Selecting needed files in Bullit BA\n",
    "Bullit_TRANSCAD_FILE_BA_2030 = Bullit_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_BA_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_BA_2030 = Bullit_HPMS_FACTORS_BA_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_BA_2030['BA_FUNCL'] = Bullit_TRANSCAD_FILE_BA_2030['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_BA_2030['BA_FUNCL'] = Bullit_HPMS_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2030['BA_FUNCL'] = HER_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "\n",
    "\n",
    "# Create the adjusted volume of for the month of July using the HPMS file. \n",
    "Bullit_July_volume_BA_2030 = Bullit_TRANSCAD_FILE_BA_2030.merge(Bullit_HPMS_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "Bullit_July_volume_BA_2030['BA_July_Volume'] = Bullit_July_volume_BA_2030.BA_VOL_Fin * Bullit_July_volume_BA_2030.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_BA_2030 = Bullit_July_volume_BA_2030.merge(ATR_HOURLY_VOLUME_BA_2030, on='BA_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_BA_2030 = Bullit_July_volume_BA_2030.merge(HER_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_BA_2030 = Bullit_July_volume_BA_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\",'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_BA_2030 = Bullit_July_volume_BA_2030.loc[:, [\"ID\", \"BA_FUNCL\",'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_BA_2030 = Bullit_July_24hr_matrix_BA_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_BA_2030[\"BA_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_BA_2030 = Bullit_July_24hr_matrix_BA_2030.fillna(0)\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_BA_2030 = pd.concat([Bullit_July_volume_Only_BA_2030,Bullit_July_24hr_matrix_BA_2030 ], axis=1, join='inner')\n",
    "\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for Bullitt County\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_BA_2030)\n",
    "\n",
    "# Bullitt_July_Hourly_Volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2030(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2030 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2030\n",
    "\n",
    "# Call the function for AB direction in Bullit\n",
    "funcl_summary_Bullit_ab_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Bullit_July_Hourly_Volume_2030, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Bullit\n",
    "funcl_summary_Bullit_ba_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Bullit_July_Hourly_Volume_BA_2030, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Bullit_ab_2030 = funcl_summary_Bullit_ab_2030.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ab_2030 = funcl_summary_Bullit_ab_2030.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Bullit_ba_2030 = funcl_summary_Bullit_ba_2030.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ba_2030 = funcl_summary_Bullit_ba_2030.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2030 = pd.merge(funcl_summary_Bullit_ab_2030, funcl_summary_Bullit_ba_2030, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2030['Total_NEW_VMT'] = combined_summary_2030['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2030['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2030['Total_VHT'] = combined_summary_2030['Total_VHT_AB'].fillna(0) + combined_summary_2030['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2030 = combined_summary_2030['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2030 = combined_summary_2030['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2030['Mean_Congested_Speed'] = (mean_speed_ab_2030 + mean_speed_ba_2030) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Bullit\n",
    "Bullit_VMT_2030 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2030['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2030['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2030['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2030['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Bullit\n",
    "#print(Bullit_VMT)\n",
    "#Bullit_VMT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming Bullit_VMT is defined elsewhere in your code\n",
    "Bullit_VMT_df_2030 = pd.DataFrame(Bullit_VMT_2030)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\", \n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Bullit_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Bullit_VMT_df_2030 = pd.merge(expected_Bullit_VMT_df, Bullit_VMT_df_2030, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Bullit_VMT_df_2030[\"Total_NEW_VMT\"] = final_Bullit_VMT_df_2030[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Bullit_VMT_df_2030[\"Total_VHT\"] = final_Bullit_VMT_df_2030[\"Total_VHT\"].astype(float)\n",
    "final_Bullit_VMT_df_2030[\"Mean_Congested_Speed\"] = final_Bullit_VMT_df_2030[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "#final_Bullit_VMT_df_2030\n",
    "\n",
    "\n",
    "#4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Bullit_VMT_df_2030[final_Bullit_VMT_df_2030['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Bullit_VMT_df_2030[final_Bullit_VMT_df_2030['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Bullit_VMT_df_2030 = pd.concat([final_Bullit_VMT_df_2030, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Bullit_VMT_df_2030['Speed'] = final_Bullit_VMT_df_2030['Total_NEW_VMT'] / final_Bullit_VMT_df_2030['Total_VHT']\n",
    "\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Bullit_VMT_df_2030) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2030, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2030, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2030, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2030, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a4d4b9e-6e0e-4367-ae2d-a1f4ee92db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Bullit_VMT_df_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f038541-18ec-4f4b-8c62-dc47a6b5f4d4",
   "metadata": {},
   "source": [
    "# 2030 Clark County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10540c9d-b891-4da0-a48d-c57bd5dbb758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2407523197.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Clark \n",
    "# Selecting needed files in Clark AB\n",
    "Clark_TRANSCAD_FILE_AB_2030 = Clark_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_AB_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_AB_2030 = Clark_HPMS_FACTORS_AB_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_AB_2030['AB_FUNCL'] = Clark_TRANSCAD_FILE_AB_2030['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_AB_2030['AB_FUNCL'] = Clark_HPMS_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2030['AB_FUNCL'] = HER_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_2030 = Clark_TRANSCAD_FILE_AB_2030.merge(Clark_HPMS_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "Clark_July_volume_2030['AB_July_Volume'] = Clark_July_volume_2030.AB_VOL_Fin * Clark_July_volume_2030.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_2030 = Clark_July_volume_2030.merge(ATR_HOURLY_VOLUME_AB_2030, on='AB_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_2030 = Clark_July_volume_2030.merge(HER_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_2030 = Clark_July_volume_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_2030 = Clark_July_volume_2030.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_2030 = Clark_July_24hr_matrix_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_2030[\"AB_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_2030 = Clark_July_24hr_matrix_2030.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_2030 = pd.concat([Clark_July_volume_Only_2030, Clark_July_24hr_matrix_2030], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Clark_July_Hourly_Volume_2030\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_2030)\n",
    "\n",
    "\n",
    "#2\n",
    "# Selecting needed files in Clark BA\n",
    "Clark_TRANSCAD_FILE_BA_2030 = Clark_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_BA_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_BA_2030 = Clark_HPMS_FACTORS_BA_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_BA_2030['BA_FUNCL'] = Clark_TRANSCAD_FILE_BA_2030['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_BA_2030['BA_FUNCL'] = Clark_HPMS_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2030['BA_FUNCL'] = HER_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_BA_2030 = Clark_TRANSCAD_FILE_BA_2030.merge(Clark_HPMS_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "Clark_July_volume_BA_2030['BA_July_Volume'] = Clark_July_volume_BA_2030.BA_VOL_Fin * Clark_July_volume_BA_2030.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_BA_2030 = Clark_July_volume_BA_2030.merge(ATR_HOURLY_VOLUME_BA_2030, on='BA_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_BA_2030 = Clark_July_volume_BA_2030.merge(HER_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_BA_2030 = Clark_July_volume_BA_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_BA_2030 = Clark_July_volume_BA_2030.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_BA_2030 = Clark_July_24hr_matrix_BA_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_BA_2030[\"BA_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_BA_2030 = Clark_July_24hr_matrix_BA_2030.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_BA_2030 = pd.concat([Clark_July_volume_Only_BA_2030, Clark_July_24hr_matrix_BA_2030], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_BA_2030)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2030(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2030 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2030\n",
    "\n",
    "# Call the function for AB direction in Clark\n",
    "funcl_summary_Clark_ab_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Clark_July_Hourly_Volume_2030, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Clark\n",
    "funcl_summary_Clark_ba_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Clark_July_Hourly_Volume_BA_2030, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Clark_ab_2030 = funcl_summary_Clark_ab_2030.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ab_2030 = funcl_summary_Clark_ab_2030.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Clark_ba_2030 = funcl_summary_Clark_ba_2030.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ba_2030 = funcl_summary_Clark_ba_2030.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2030 = pd.merge(funcl_summary_Clark_ab_2030, funcl_summary_Clark_ba_2030, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2030['Total_NEW_VMT'] = combined_summary_2030['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2030['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2030['Total_VHT'] = combined_summary_2030['Total_VHT_AB'].fillna(0) + combined_summary_2030['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2030 = combined_summary_2030['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2030 = combined_summary_2030['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2030['Mean_Congested_Speed'] = (mean_speed_ab_2030 + mean_speed_ba_2030) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Clark\n",
    "Clark_VMT_2030 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2030['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2030['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2030['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2030['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Clark\n",
    "#print(Clark_VMT)\n",
    "#Clark_VMT\n",
    "\n",
    "# Assuming Clark_VMT is defined elsewhere in your code\n",
    "Clark_VMT_df_2030 = pd.DataFrame(Clark_VMT_2030)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Clark_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Clark_VMT_df_2030 = pd.merge(expected_Clark_VMT_df, Clark_VMT_df_2030, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Clark_VMT_df_2030[\"Total_NEW_VMT\"] = final_Clark_VMT_df_2030[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Clark_VMT_df_2030[\"Total_VHT\"] = final_Clark_VMT_df_2030[\"Total_VHT\"].astype(float)\n",
    "final_Clark_VMT_df_2030[\"Mean_Congested_Speed\"] = final_Clark_VMT_df_2030[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "#4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Clark_VMT_df_2030[final_Clark_VMT_df_2030['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Clark_VMT_df_2030[final_Clark_VMT_df_2030['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Clark_VMT_df_2030 = pd.concat([final_Clark_VMT_df_2030, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Clark_VMT_df_2030['Speed'] = final_Clark_VMT_df_2030['Total_NEW_VMT'] / final_Clark_VMT_df_2030['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Clark_VMT_df_2030) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2030, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2030, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2030, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2030, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Clark_VMT_df_2030\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44da678e-3c4e-46e3-9eef-b06268b9bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Clark_VMT_df_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3880e-2310-4f68-9a16-4faea924614b",
   "metadata": {},
   "source": [
    "# 2030 Floyd County processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6a2ff22-6bb7-446a-8aa1-6d33283feab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:192: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:192: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:337: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] +\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1508761975.py:350: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n"
     ]
    }
   ],
   "source": [
    "# Floyd \n",
    "# Selecting needed files in Floyd AB\n",
    "Floyd_TRANSCAD_FILE_AB_2030 = Floyd_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_AB_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_AB_2030 = Floyd_HPMS_FACTORS_AB_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_AB_2030['AB_FUNCL'] = Floyd_TRANSCAD_FILE_AB_2030['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_AB_2030['AB_FUNCL'] = Floyd_HPMS_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2030['AB_FUNCL'] = HER_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_2030 = Floyd_TRANSCAD_FILE_AB_2030.merge(Floyd_HPMS_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "Floyd_July_volume_2030['AB_July_Volume'] = Floyd_July_volume_2030.AB_VOL_Fin * Floyd_July_volume_2030.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_2030 = Floyd_July_volume_2030.merge(ATR_HOURLY_VOLUME_AB_2030, on='AB_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_2030 = Floyd_July_volume_2030.merge(HER_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_2030 = Floyd_July_volume_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_2030 = Floyd_July_volume_2030.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_2030 = Floyd_July_24hr_matrix_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_2030[\"AB_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_2030 = Floyd_July_24hr_matrix_2030.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_2030 = pd.concat([Floyd_July_volume_Only_2030, Floyd_July_24hr_matrix_2030], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Floyd_July_Hourly_Volume_2030\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_2030)\n",
    "\n",
    "\n",
    "# Selecting needed files in Floyd BA\n",
    "Floyd_TRANSCAD_FILE_BA_2030 = Floyd_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_BA_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_BA_2030 = Floyd_HPMS_FACTORS_BA_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_BA_2030['BA_FUNCL'] = Floyd_TRANSCAD_FILE_BA_2030['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_BA_2030['BA_FUNCL'] = Floyd_HPMS_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2030['BA_FUNCL'] = HER_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_BA_2030 = Floyd_TRANSCAD_FILE_BA_2030.merge(Floyd_HPMS_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "Floyd_July_volume_BA_2030['BA_July_Volume'] = Floyd_July_volume_BA_2030.BA_VOL_Fin * Floyd_July_volume_BA_2030.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_BA_2030 = Floyd_July_volume_BA_2030.merge(ATR_HOURLY_VOLUME_BA_2030, on='BA_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_BA_2030 = Floyd_July_volume_BA_2030.merge(HER_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_BA_2030 = Floyd_July_volume_BA_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_BA_2030 = Floyd_July_volume_BA_2030.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_BA_2030 = Floyd_July_24hr_matrix_BA_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_BA_2030[\"BA_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_BA_2030 = Floyd_July_24hr_matrix_BA_2030.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_BA_2030 = pd.concat([Floyd_July_volume_Only_BA_2030, Floyd_July_24hr_matrix_BA_2030], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section in case of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_BA_2030)\n",
    "\n",
    "# Save results if needed Copy this section in case of error\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2030(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2030 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2030\n",
    "\n",
    "# Call the function for AB direction in Floyd\n",
    "funcl_summary_Floyd_ab_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Floyd_July_Hourly_Volume_2030, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Floyd\n",
    "funcl_summary_Floyd_ba_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Floyd_July_Hourly_Volume_BA_2030, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Floyd_ab_2030 = funcl_summary_Floyd_ab_2030.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ab_2030 = funcl_summary_Floyd_ab_2030.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Floyd_ba_2030 = funcl_summary_Floyd_ba_2030.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ba_2030 = funcl_summary_Floyd_ba_2030.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2030 = pd.merge(funcl_summary_Floyd_ab_2030, funcl_summary_Floyd_ba_2030, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2030['Total_NEW_VMT'] = combined_summary_2030['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2030['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2030['Total_VHT'] = combined_summary_2030['Total_VHT_AB'].fillna(0) + combined_summary_2030['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2030 = combined_summary_2030['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2030 = combined_summary_2030['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2030['Mean_Congested_Speed'] = (mean_speed_ab_2030 + mean_speed_ba_2030) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Floyd\n",
    "Floyd_VMT_2030 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2030['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2030['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2030['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2030['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Floyd\n",
    "#print(Floyd_VMT)\n",
    "#Floyd_VMT\n",
    "\n",
    "# Assuming Floyd_VMT is defined elsewhere in your code\n",
    "Floyd_VMT_df_2030 = pd.DataFrame(Floyd_VMT_2030)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Floyd_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Floyd_VMT_df_2030 = pd.merge(expected_Floyd_VMT_df, Floyd_VMT_df_2030, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Floyd_VMT_df_2030[\"Total_NEW_VMT\"] = final_Floyd_VMT_df_2030[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Floyd_VMT_df_2030[\"Total_VHT\"] = final_Floyd_VMT_df_2030[\"Total_VHT\"].astype(float)\n",
    "final_Floyd_VMT_df_2030[\"Mean_Congested_Speed\"] = final_Floyd_VMT_df_2030[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "#4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Floyd_VMT_df_2030[final_Floyd_VMT_df_2030['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Floyd_VMT_df_2030[final_Floyd_VMT_df_2030['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Floyd_VMT_df_2030 = pd.concat([final_Floyd_VMT_df_2030, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Floyd_VMT_df_2030['Speed'] = final_Floyd_VMT_df_2030['Total_NEW_VMT'] / final_Floyd_VMT_df_2030['Total_VHT']\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Floyd_VMT_df_2030) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2030, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2030, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2030, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2030, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#final_Floyd_VMT_df_2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5842107a-67fe-4e7f-90d4-e99c0c652295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Floyd_VMT_df_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db79425-f2cf-48b0-b66d-55a60e635fe3",
   "metadata": {},
   "source": [
    "# 2030 Jefferson County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baf1576e-4b3d-45ae-abf6-f11441af8be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\983358211.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Jefferson \n",
    "# Selecting needed files in Jefferson AB\n",
    "Jefferson_TRANSCAD_FILE_AB_2030 = Jefferson_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                     'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                     'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                     'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_AB_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_AB_2030 = Jefferson_HPMS_FACTORS_AB_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_AB_2030['AB_FUNCL'] = Jefferson_TRANSCAD_FILE_AB_2030['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_AB_2030['AB_FUNCL'] = Jefferson_HPMS_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2030['AB_FUNCL'] = HER_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_2030 = Jefferson_TRANSCAD_FILE_AB_2030.merge(Jefferson_HPMS_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "Jefferson_July_volume_2030['AB_July_Volume'] = Jefferson_July_volume_2030.AB_VOL_Fin * Jefferson_July_volume_2030.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_2030 = Jefferson_July_volume_2030.merge(ATR_HOURLY_VOLUME_AB_2030, on='AB_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_2030 = Jefferson_July_volume_2030.merge(HER_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_2030 = Jefferson_July_volume_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                     \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                     \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_2030 = Jefferson_July_volume_2030.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                     'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                     'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                     'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                     'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                     'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                     'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_2030 = Jefferson_July_24hr_matrix_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                    'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                    'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                    'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                    'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                    'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_2030[\"AB_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_2030 = Jefferson_July_24hr_matrix_2030.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2030 = pd.concat([Jefferson_July_volume_Only_2030, Jefferson_July_24hr_matrix_2030], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2030\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_2030)\n",
    "\n",
    "\n",
    "# Selecting needed files in Jefferson BA\n",
    "Jefferson_TRANSCAD_FILE_BA_2030 = Jefferson_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                       'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                       'BA_CAPDAY', \n",
    "                                                                       'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_BA_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_BA_2030 = Jefferson_HPMS_FACTORS_BA_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_BA_2030['BA_FUNCL'] = Jefferson_TRANSCAD_FILE_BA_2030['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_BA_2030['BA_FUNCL'] = Jefferson_HPMS_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2030['BA_FUNCL'] = HER_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_BA_2030 = Jefferson_TRANSCAD_FILE_BA_2030.merge(Jefferson_HPMS_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "Jefferson_July_volume_BA_2030['BA_July_Volume'] = Jefferson_July_volume_BA_2030.BA_VOL_Fin * Jefferson_July_volume_BA_2030.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_BA_2030 = Jefferson_July_volume_BA_2030.merge(ATR_HOURLY_VOLUME_BA_2030, on='BA_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_BA_2030 = Jefferson_July_volume_BA_2030.merge(HER_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_BA_2030 = Jefferson_July_volume_BA_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                         \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                         \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_BA_2030 = Jefferson_July_volume_BA_2030.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                         'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                         'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                         'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                         'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                         'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_BA_2030 = Jefferson_July_24hr_matrix_BA_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_BA_2030[\"BA_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_BA_2030 = Jefferson_July_24hr_matrix_BA_2030.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_BA_2030 = pd.concat([Jefferson_July_volume_Only_BA_2030, Jefferson_July_24hr_matrix_BA_2030], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section in case of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_BA_2030)\n",
    "\n",
    "# Save results if needed Copy this section in case of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2030(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2030 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2030\n",
    "\n",
    "# Call the function for AB direction in Jefferson\n",
    "funcl_summary_Jefferson_ab_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Jefferson_July_Hourly_Volume_2030, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Jefferson\n",
    "funcl_summary_Jefferson_ba_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Jefferson_July_Hourly_Volume_BA_2030, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Jefferson_ab_2030 = funcl_summary_Jefferson_ab_2030.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ab_2030 = funcl_summary_Jefferson_ab_2030.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Jefferson_ba_2030 = funcl_summary_Jefferson_ba_2030.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ba_2030 = funcl_summary_Jefferson_ba_2030.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2030 = pd.merge(funcl_summary_Jefferson_ab_2030, funcl_summary_Jefferson_ba_2030, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2030['Total_NEW_VMT'] = combined_summary_2030['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2030['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2030['Total_VHT'] = combined_summary_2030['Total_VHT_AB'].fillna(0) + combined_summary_2030['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2030 = combined_summary_2030['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2030 = combined_summary_2030['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2030['Mean_Congested_Speed'] = (mean_speed_ab_2030 + mean_speed_ba_2030) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Jefferson\n",
    "Jefferson_VMT_2030 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2030['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2030['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2030['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2030['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Jefferson\n",
    "#print(Jefferson_VMT)\n",
    "#Jefferson_VMT\n",
    "\n",
    "# Assuming Jefferson_VMT is defined elsewhere in your code\n",
    "Jefferson_VMT_df_2030 = pd.DataFrame(Jefferson_VMT_2030)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Jefferson_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Jefferson_VMT_df_2030 = pd.merge(expected_Jefferson_VMT_df, Jefferson_VMT_df_2030, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Jefferson_VMT_df_2030[\"Total_NEW_VMT\"] = final_Jefferson_VMT_df_2030[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2030[\"Total_VHT\"] = final_Jefferson_VMT_df_2030[\"Total_VHT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2030[\"Mean_Congested_Speed\"] = final_Jefferson_VMT_df_2030[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "#4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Jefferson_VMT_df_2030[final_Jefferson_VMT_df_2030['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Jefferson_VMT_df_2030[final_Jefferson_VMT_df_2030['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Jefferson_VMT_df_2030 = pd.concat([final_Jefferson_VMT_df_2030, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Jefferson_VMT_df_2030['Speed'] = final_Jefferson_VMT_df_2030['Total_NEW_VMT'] / final_Jefferson_VMT_df_2030['Total_VHT']\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Jefferson_VMT_df_2030) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2030, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2030, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2030, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2030, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Jefferson_VMT_df_2030\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be5f22f0-ed7b-4c06-94f2-04b4bd31510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Jefferson_VMT_df_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0517b41-ecdd-4276-864a-a93b3e50b63c",
   "metadata": {},
   "source": [
    "# 2030 Oldham County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "882c23a8-6ccd-49dc-a8db-e5f08ef18cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\4079020910.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Oldham \n",
    "# Selecting needed files in Oldham AB\n",
    "Oldham_TRANSCAD_FILE_AB_2030 = Oldham_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                  'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                  'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_AB_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_AB_2030 = Oldham_HPMS_FACTORS_AB_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_AB_2030['AB_FUNCL'] = Oldham_TRANSCAD_FILE_AB_2030['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2030['AB_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_AB_2030['AB_FUNCL'] = Oldham_HPMS_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2030['AB_FUNCL'] = HER_FACTORS_AB_2030['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_2030 = Oldham_TRANSCAD_FILE_AB_2030.merge(Oldham_HPMS_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "Oldham_July_volume_2030['AB_July_Volume'] = Oldham_July_volume_2030.AB_VOL_Fin * Oldham_July_volume_2030.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_2030 = Oldham_July_volume_2030.merge(ATR_HOURLY_VOLUME_AB_2030, on='AB_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_2030 = Oldham_July_volume_2030.merge(HER_FACTORS_AB_2030, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_2030 = Oldham_July_volume_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_2030 = Oldham_July_volume_2030.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_2030 = Oldham_July_24hr_matrix_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                               'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                               'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                               'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                               'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                               'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_2030[\"AB_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_2030 = Oldham_July_24hr_matrix_2030.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_2030 = pd.concat([Oldham_July_volume_Only_2030, Oldham_July_24hr_matrix_2030], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Oldham_July_Hourly_Volume_2030\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_2030)\n",
    "\n",
    "\n",
    "# Selecting needed files in Oldham BA\n",
    "Oldham_TRANSCAD_FILE_BA_2030 = Oldham_TRANSCAD_FILE_2030.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                  'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                  'BA_CAPDAY', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2030 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_BA_2030 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_BA_2030 = Oldham_HPMS_FACTORS_BA_2030.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2030 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_BA_2030['BA_FUNCL'] = Oldham_TRANSCAD_FILE_BA_2030['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2030['BA_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_BA_2030['BA_FUNCL'] = Oldham_HPMS_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2030['BA_FUNCL'] = HER_FACTORS_BA_2030['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_BA_2030 = Oldham_TRANSCAD_FILE_BA_2030.merge(Oldham_HPMS_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "Oldham_July_volume_BA_2030['BA_July_Volume'] = Oldham_July_volume_BA_2030.BA_VOL_Fin * Oldham_July_volume_BA_2030.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_BA_2030 = Oldham_July_volume_BA_2030.merge(ATR_HOURLY_VOLUME_BA_2030, on='BA_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_BA_2030 = Oldham_July_volume_BA_2030.merge(HER_FACTORS_BA_2030, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_BA_2030 = Oldham_July_volume_BA_2030.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                      \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                      \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_BA_2030 = Oldham_July_volume_BA_2030.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                      'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                      'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                      'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                      'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                      'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                      'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_BA_2030 = Oldham_July_24hr_matrix_BA_2030[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                     'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                     'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                     'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                     'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                     'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_BA_2030[\"BA_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_BA_2030 = Oldham_July_24hr_matrix_BA_2030.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_BA_2030 = pd.concat([Oldham_July_volume_Only_BA_2030, Oldham_July_24hr_matrix_BA_2030], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section in case of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_BA_2030)\n",
    "\n",
    "# Save results if needed Copy this section in case of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2030(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2030 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2030 = pd.concat([summary_df_2030, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2030\n",
    "\n",
    "# Call the function for AB direction in Oldham\n",
    "funcl_summary_Oldham_ab_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Oldham_July_Hourly_Volume_2030, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Oldham\n",
    "funcl_summary_Oldham_ba_2030 = sum_new_vmt_vht_and_speed_by_direction_2030(Oldham_July_Hourly_Volume_BA_2030, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Oldham_ab_2030 = funcl_summary_Oldham_ab_2030.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ab_2030 = funcl_summary_Oldham_ab_2030.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Oldham_ba_2030 = funcl_summary_Oldham_ba_2030.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ba_2030 = funcl_summary_Oldham_ba_2030.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2030 = pd.merge(funcl_summary_Oldham_ab_2030, funcl_summary_Oldham_ba_2030, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2030['Total_NEW_VMT'] = combined_summary_2030['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2030['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2030['Total_VHT'] = combined_summary_2030['Total_VHT_AB'].fillna(0) + combined_summary_2030['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2030 = combined_summary_2030['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2030 = combined_summary_2030['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2030['Mean_Congested_Speed'] = (mean_speed_ab_2030 + mean_speed_ba_2030) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Oldham\n",
    "Oldham_VMT_2030 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2030['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2030['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2030['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2030['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Oldham\n",
    "#print(Oldham_VMT)\n",
    "#Oldham_VMT\n",
    "\n",
    "# Assuming Oldham_VMT is defined elsewhere in your code\n",
    "Oldham_VMT_df_2030 = pd.DataFrame(Oldham_VMT_2030)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Oldham_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Oldham_VMT_df_2030 = pd.merge(expected_Oldham_VMT_df, Oldham_VMT_df_2030, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Oldham_VMT_df_2030[\"Total_NEW_VMT\"] = final_Oldham_VMT_df_2030[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Oldham_VMT_df_2030[\"Total_VHT\"] = final_Oldham_VMT_df_2030[\"Total_VHT\"].astype(float)\n",
    "final_Oldham_VMT_df_2030[\"Mean_Congested_Speed\"] = final_Oldham_VMT_df_2030[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "#4\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Oldham_VMT_df_2030[final_Oldham_VMT_df_2030['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Oldham_VMT_df_2030[final_Oldham_VMT_df_2030['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Oldham_VMT_df_2030 = pd.concat([final_Oldham_VMT_df_2030, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Oldham_VMT_df_2030['Speed'] = final_Oldham_VMT_df_2030['Total_NEW_VMT'] / final_Oldham_VMT_df_2030['Total_VHT']\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Oldham_VMT_df_2030) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2030, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2030, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2030, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2030 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2030, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Oldham_VMT_df_2030\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3becec2-5b88-4ca9-8f8a-51bfcc024b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Oldham_VMT_df_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24604b49-9801-4614-ae66-1cf64a2ed5da",
   "metadata": {},
   "source": [
    "# Combine all 2030 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3cae6ea-a0a8-4a4b-b535-0011e034f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Create a blank DataFrame with the same columns to use for spacing\n",
    "blank_lines = pd.DataFrame(np.nan, index=range(5), columns=Clark_VMT.columns)\n",
    "\n",
    "# Initialize a list to hold the DataFrames and their titles\n",
    "dataframes_with_titles = []\n",
    "\n",
    "# Append each DataFrame with its title and blank lines\n",
    "for county, df in zip(['Bullit','Clark', 'Floyd', 'Jefferson', 'Oldham'], \n",
    "                      [final_Bullit_VMT_df_2030, final_Clark_VMT_df_2030, final_Floyd_VMT_df_2030, final_Jefferson_VMT_df_2030, final_Oldham_VMT_df_2030]):\n",
    "    # Create a DataFrame for the title (bold)\n",
    "    title_df = pd.DataFrame(columns=df.columns)\n",
    "    title_df.loc[0] = df.columns  # Set the title row\n",
    "    # Append the county name with \"County\" as the title\n",
    "    dataframes_with_titles.append((f\"{county} County\", title_df, df, blank_lines))\n",
    "\n",
    "# Create a new Excel workbook\n",
    "output_file_path = '2030_Combined_VMT_Data.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = 'VMT_Data'\n",
    "\n",
    "# Add the dataframes and format them\n",
    "row_offset = 0\n",
    "for title, title_df, df, blanks in dataframes_with_titles:\n",
    "    # Add the file name with \"County\" as a subheading above each DataFrame\n",
    "    ws.cell(row=row_offset + 1, column=1, value=title).font = Font(bold=True)\n",
    "    row_offset += 1  # Move to the next row for the title\n",
    "\n",
    "    # Write the column titles (bold)\n",
    "    for c_idx, column_name in enumerate(title_df.columns, start=1):\n",
    "        ws.cell(row=row_offset + 1, column=c_idx, value=column_name).font = Font(bold=True)\n",
    "\n",
    "    # Write the DataFrame data\n",
    "    for r_idx, row in enumerate(df.values, start=row_offset + 2):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Adjust column widths based on the maximum length of data in each column\n",
    "    for c_idx in range(1, df.shape[1] + 1):\n",
    "        max_length = 0\n",
    "        # Check title length\n",
    "        max_length = max(max_length, len(title_df.columns[c_idx - 1]))  \n",
    "        # Check data length\n",
    "        for r_idx in range(df.shape[0]):\n",
    "            cell_value = str(df.iat[r_idx, c_idx - 1])  # Get cell value\n",
    "            max_length = max(max_length, len(cell_value))  # Update max length if necessary\n",
    "        ws.column_dimensions[ws.cell(row=1, column=c_idx).column_letter].width = max_length + 2  # Add some padding\n",
    "\n",
    "    # Update the row_offset for the next DataFrame\n",
    "    row_offset += df.shape[0] + 3  # 2 for the data and 1 for the title\n",
    "\n",
    "    # Add blank lines\n",
    "    for _ in range(5):\n",
    "        row_offset += 1  # Add blank lines between DataFrames\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd9aea5-1879-4788-ab92-58b2ebea9806",
   "metadata": {},
   "source": [
    "# 2035 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cc1c887-aa5f-4bbc-adcb-63fbae8b227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant variables\n",
    "TRANSCAD_RESULT_FILE_2035 = TRANSCAD_RESULT_FILE_2035.loc[:, ['ID', 'Length', 'Link_AADT', \n",
    "                                             'County', 'AB_FACT', 'FUNCL', 'BA_FACT', \n",
    "                                              'AB_FFSPD', 'BA_FFSPD', 'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                              'BA_VOL_Fin', 'Tot_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Fill null values in Link_AADT and convert types\n",
    "TRANSCAD_RESULT_FILE_2035['Link_AADT'] = TRANSCAD_RESULT_FILE_2035['Link_AADT'].fillna(0)\n",
    "TRANSCAD_RESULT_FILE_2035[\"Link_AADT\"] = TRANSCAD_RESULT_FILE_2035[\"Link_AADT\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"AB_CAPDAY\"] = TRANSCAD_RESULT_FILE_2035[\"AB_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"BA_CAPDAY\"] = TRANSCAD_RESULT_FILE_2035[\"BA_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"FUNCL\"] = TRANSCAD_RESULT_FILE_2035[\"FUNCL\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"AB_FFSPD\"] = TRANSCAD_RESULT_FILE_2035[\"AB_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"BA_FFSPD\"] = TRANSCAD_RESULT_FILE_2035[\"BA_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"B_exponent\"] = TRANSCAD_RESULT_FILE_2035[\"B_exponent\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2035[\"A_coefficient\"] = TRANSCAD_RESULT_FILE_2035[\"A_coefficient\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2035[\"BA_VOL_Fin\"] = TRANSCAD_RESULT_FILE_2035[\"BA_VOL_Fin\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2035[\"Length\"] = TRANSCAD_RESULT_FILE_2035[\"Length\"].astype(float)\n",
    "\n",
    "# Calculate Peak Hour Capacity\n",
    "TRANSCAD_RESULT_FILE_2035['AB_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2035.AB_CAPDAY / 10\n",
    "TRANSCAD_RESULT_FILE_2035['BA_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2035.BA_CAPDAY / 10\n",
    "\n",
    "# Create duplicate points column\n",
    "TRANSCAD_RESULT_FILE_2035['AB_FUNCL'] = TRANSCAD_RESULT_FILE_2035.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2035['BA_FUNCL'] = TRANSCAD_RESULT_FILE_2035.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2035['AB_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2035.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2035['BA_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2035.loc[:, 'FUNCL']\n",
    "\n",
    "# Library original\n",
    "TRANSCAD_RESULT_FILE_2035[\"AB_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                   8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                   18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "# Library\n",
    "TRANSCAD_RESULT_FILE_2035[\"BA_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                  8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                  18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2035[\"AB_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2035[\"BA_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "705a4735-b623-4543-bcb1-af37ba35c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating different county files \n",
    "Bullit_TRANSCAD_FILE_2035 = TRANSCAD_RESULT_FILE_2035.loc[TRANSCAD_RESULT_FILE_2035.County == 'Bullitt', :]\n",
    "\n",
    "Clark_TRANSCAD_FILE_2035 = TRANSCAD_RESULT_FILE_2035.loc[TRANSCAD_RESULT_FILE_2035.County == 'Clark', :]\n",
    "\n",
    "Floyd_TRANSCAD_FILE_2035 = TRANSCAD_RESULT_FILE_2035.loc[TRANSCAD_RESULT_FILE_2035.County == 'Floyd', :]\n",
    "\n",
    "Jefferson_TRANSCAD_FILE_2035 = TRANSCAD_RESULT_FILE_2035.loc[TRANSCAD_RESULT_FILE_2035.County == 'Jefferson', :]\n",
    "\n",
    "Oldham_TRANSCAD_FILE_2035 = TRANSCAD_RESULT_FILE_2035.loc[TRANSCAD_RESULT_FILE_2035.County == 'Oldham', :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6ec62-bc01-429c-b8da-443dcb609dd4",
   "metadata": {},
   "source": [
    "# 2035 Bullit Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da95f2a8-9773-4738-b314-adf505de0c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3082606216.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Bullit \n",
    "# Selecting needed files in Bullit AB\n",
    "Bullit_TRANSCAD_FILE_AB_2035 = Bullit_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity', \n",
    "                                                           'AB_FUNCL', 'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','AB_FFSPD', 'BA_FFSPD', \n",
    "                                                           'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_AB_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_AB_2035 = Bullit_HPMS_FACTORS_AB_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_AB_2035['AB_FUNCL'] = Bullit_TRANSCAD_FILE_AB_2035['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_AB_2035['AB_FUNCL'] = Bullit_HPMS_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2035['AB_FUNCL'] = HER_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume_2035 = Bullit_TRANSCAD_FILE_AB_2035.merge(Bullit_HPMS_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "Bullit_July_volume_2035['AB_July_Volume'] = Bullit_July_volume_2035.AB_VOL_Fin * Bullit_July_volume_2035.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_2035 = Bullit_July_volume_2035.merge(ATR_HOURLY_VOLUME_AB_2035, on='AB_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_2035 = Bullit_July_volume_2035.merge(HER_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_2035 = Bullit_July_volume_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_2035 = Bullit_July_volume_2035.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_2035 = Bullit_July_24hr_matrix_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_2035[\"AB_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_2035 = Bullit_July_24hr_matrix_2035.fillna(0)\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_2035 = pd.concat([Bullit_July_volume_Only_2035,Bullit_July_24hr_matrix_2035 ], axis=1, join='inner')\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_2035)\n",
    "\n",
    "\n",
    "\n",
    "# Selecting needed files in Bullit BA\n",
    "Bullit_TRANSCAD_FILE_BA_2035 = Bullit_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                           'BA_FUNCL','AB_FUNCL_ATR','BA_FUNCL_ATR','BA_FFSPD', \n",
    "                                                           'BA_CAPDAY', \n",
    "                                                           'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_BA_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_BA_2035 = Bullit_HPMS_FACTORS_BA_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_BA_2035['BA_FUNCL'] = Bullit_TRANSCAD_FILE_BA_2035['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_BA_2035['BA_FUNCL'] = Bullit_HPMS_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2035['BA_FUNCL'] = HER_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "\n",
    "\n",
    "# Create the adjusted volume of for the month of July using the HPMS file. \n",
    "Bullit_July_volume_BA_2035 = Bullit_TRANSCAD_FILE_BA_2035.merge(Bullit_HPMS_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "Bullit_July_volume_BA_2035['BA_July_Volume'] = Bullit_July_volume_BA_2035.BA_VOL_Fin * Bullit_July_volume_BA_2035.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_BA_2035 = Bullit_July_volume_BA_2035.merge(ATR_HOURLY_VOLUME_BA_2035, on='BA_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_BA_2035 = Bullit_July_volume_BA_2035.merge(HER_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_BA_2035 = Bullit_July_volume_BA_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\",'BA_FUNCL_ATR', \n",
    "                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_BA_2035 = Bullit_July_volume_BA_2035.loc[:, [\"ID\", \"BA_FUNCL\",'BA_FUNCL_ATR', \"BA_July_Volume\", \n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_BA_2035 = Bullit_July_24hr_matrix_BA_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_BA_2035[\"BA_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_BA_2035 = Bullit_July_24hr_matrix_BA_2035.fillna(0)\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_BA_2035 = pd.concat([Bullit_July_volume_Only_BA_2035, Bullit_July_24hr_matrix_BA_2035], axis=1, join='inner')\n",
    "\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for Bullitt County\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_BA_2035)\n",
    "\n",
    "# Bullitt_July_Hourly_Volume\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2035(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2035 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2035\n",
    "\n",
    "# Call the function for AB direction in Bullit\n",
    "funcl_summary_Bullit_ab_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Bullit_July_Hourly_Volume_2035, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Bullit\n",
    "funcl_summary_Bullit_ba_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Bullit_July_Hourly_Volume_BA_2035, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Bullit_ab_2035 = funcl_summary_Bullit_ab_2035.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ab_2035 = funcl_summary_Bullit_ab_2035.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Bullit_ba_2035 = funcl_summary_Bullit_ba_2035.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ba_2035 = funcl_summary_Bullit_ba_2035.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2035 = pd.merge(funcl_summary_Bullit_ab_2035, funcl_summary_Bullit_ba_2035, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2035['Total_NEW_VMT'] = combined_summary_2035['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2035['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2035['Total_VHT'] = combined_summary_2035['Total_VHT_AB'].fillna(0) + combined_summary_2035['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2035 = combined_summary_2035['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2035 = combined_summary_2035['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2035['Mean_Congested_Speed'] = (mean_speed_ab_2035 + mean_speed_ba_2035) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Bullit\n",
    "Bullit_VMT_2035 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2035['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2035['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2035['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2035['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Bullit\n",
    "#print(Bullit_VMT)\n",
    "#Bullit_VMT\n",
    "\n",
    "# Assuming Bullit_VMT is defined elsewhere in your code\n",
    "Bullit_VMT_df_2035 = pd.DataFrame(Bullit_VMT_2035)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\", \n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Bullit_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Bullit_VMT_df_2035 = pd.merge(expected_Bullit_VMT_df, Bullit_VMT_df_2035, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Bullit_VMT_df_2035[\"Total_NEW_VMT\"] = final_Bullit_VMT_df_2035[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Bullit_VMT_df_2035[\"Total_VHT\"] = final_Bullit_VMT_df_2035[\"Total_VHT\"].astype(float)\n",
    "final_Bullit_VMT_df_2035[\"Mean_Congested_Speed\"] = final_Bullit_VMT_df_2035[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "#final_Bullit_VMT_df_2035\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Bullit_VMT_df_2035[final_Bullit_VMT_df_2035['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Bullit_VMT_df_2035[final_Bullit_VMT_df_2035['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Bullit_VMT_df_2035 = pd.concat([final_Bullit_VMT_df_2035, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Bullit_VMT_df_2035['Speed'] = final_Bullit_VMT_df_2035['Total_NEW_VMT'] / final_Bullit_VMT_df_2035['Total_VHT']\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Bullit_VMT_df_2035) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2035, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2035, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2035, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2035, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da07b328-03f0-4cda-a2d1-21a70489fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Bullit_VMT_df_2035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aff2a6-ac15-4ff4-9295-4a7178904db9",
   "metadata": {},
   "source": [
    "# 2035 Clark Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d80d57b1-4b89-458a-ac3f-45dc31a6d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3605411274.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Clark \n",
    "# Selecting needed files in Clark AB\n",
    "Clark_TRANSCAD_FILE_AB_2035 = Clark_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_AB_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_AB_2035 = Clark_HPMS_FACTORS_AB_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_AB_2035['AB_FUNCL'] = Clark_TRANSCAD_FILE_AB_2035['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_AB_2035['AB_FUNCL'] = Clark_HPMS_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2035['AB_FUNCL'] = HER_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_2035 = Clark_TRANSCAD_FILE_AB_2035.merge(Clark_HPMS_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "Clark_July_volume_2035['AB_July_Volume'] = Clark_July_volume_2035.AB_VOL_Fin * Clark_July_volume_2035.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_2035 = Clark_July_volume_2035.merge(ATR_HOURLY_VOLUME_AB_2035, on='AB_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_2035 = Clark_July_volume_2035.merge(HER_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_2035 = Clark_July_volume_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_2035 = Clark_July_volume_2035.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_2035 = Clark_July_24hr_matrix_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_2035[\"AB_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_2035 = Clark_July_24hr_matrix_2035.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_2035 = pd.concat([Clark_July_volume_Only_2035, Clark_July_24hr_matrix_2035], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Clark_July_Hourly_Volume_2035\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_2035)\n",
    "\n",
    "\n",
    "# Selecting needed files in Clark BA\n",
    "Clark_TRANSCAD_FILE_BA_2035 = Clark_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_BA_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_BA_2035 = Clark_HPMS_FACTORS_BA_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_BA_2035['BA_FUNCL'] = Clark_TRANSCAD_FILE_BA_2035['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_BA_2035['BA_FUNCL'] = Clark_HPMS_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2035['BA_FUNCL'] = HER_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_BA_2035 = Clark_TRANSCAD_FILE_BA_2035.merge(Clark_HPMS_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "Clark_July_volume_BA_2035['BA_July_Volume'] = Clark_July_volume_BA_2035.BA_VOL_Fin * Clark_July_volume_BA_2035.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_BA_2035 = Clark_July_volume_BA_2035.merge(ATR_HOURLY_VOLUME_BA_2035, on='BA_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_BA_2035 = Clark_July_volume_BA_2035.merge(HER_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_BA_2035 = Clark_July_volume_BA_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_BA_2035 = Clark_July_volume_BA_2035.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_BA_2035 = Clark_July_24hr_matrix_BA_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_BA_2035[\"BA_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_BA_2035 = Clark_July_24hr_matrix_BA_2035.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_BA_2035 = pd.concat([Clark_July_volume_Only_BA_2035, Clark_July_24hr_matrix_BA_2035], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_BA_2035)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2035(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2035 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2035\n",
    "\n",
    "# Call the function for AB direction in Clark\n",
    "funcl_summary_Clark_ab_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Clark_July_Hourly_Volume_2035, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Clark\n",
    "funcl_summary_Clark_ba_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Clark_July_Hourly_Volume_BA_2035, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Clark_ab_2035 = funcl_summary_Clark_ab_2035.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ab_2035 = funcl_summary_Clark_ab_2035.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Clark_ba_2035 = funcl_summary_Clark_ba_2035.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ba_2035 = funcl_summary_Clark_ba_2035.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2035 = pd.merge(funcl_summary_Clark_ab_2035, funcl_summary_Clark_ba_2035, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2035['Total_NEW_VMT'] = combined_summary_2035['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2035['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2035['Total_VHT'] = combined_summary_2035['Total_VHT_AB'].fillna(0) + combined_summary_2035['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2035 = combined_summary_2035['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2035 = combined_summary_2035['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2035['Mean_Congested_Speed'] = (mean_speed_ab_2035 + mean_speed_ba_2035) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Clark\n",
    "Clark_VMT_2035 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2035['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2035['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2035['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2035['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Clark\n",
    "#print(Clark_VMT)\n",
    "#Clark_VMT\n",
    "\n",
    "# Assuming Clark_VMT is defined elsewhere in your code\n",
    "Clark_VMT_df_2035 = pd.DataFrame(Clark_VMT_2035)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Clark_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Clark_VMT_df_2035 = pd.merge(expected_Clark_VMT_df, Clark_VMT_df_2035, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Clark_VMT_df_2035[\"Total_NEW_VMT\"] = final_Clark_VMT_df_2035[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Clark_VMT_df_2035[\"Total_VHT\"] = final_Clark_VMT_df_2035[\"Total_VHT\"].astype(float)\n",
    "final_Clark_VMT_df_2035[\"Mean_Congested_Speed\"] = final_Clark_VMT_df_2035[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Clark_VMT_df_2035[final_Clark_VMT_df_2035['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Clark_VMT_df_2035[final_Clark_VMT_df_2035['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Clark_VMT_df_2035 = pd.concat([final_Clark_VMT_df_2035, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Clark_VMT_df_2035['Speed'] = final_Clark_VMT_df_2035['Total_NEW_VMT'] / final_Clark_VMT_df_2035['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Clark_VMT_df_2035) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2035, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2035, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2035, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2035, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Clark_VMT_df_2035\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24320f84-cfb2-433e-be70-909af0a5acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Clark_VMT_df_2035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e191a21-37c4-4d60-8624-1ea86593fc91",
   "metadata": {},
   "source": [
    "# 2035 Floyd Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a48a82d-e97f-48a0-86a7-5259a059f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:340: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] +\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:352: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3337058924.py:353: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n"
     ]
    }
   ],
   "source": [
    "# Floyd \n",
    "# Selecting needed files in Floyd AB\n",
    "Floyd_TRANSCAD_FILE_AB_2035 = Floyd_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_AB_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_AB_2035 = Floyd_HPMS_FACTORS_AB_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_AB_2035['AB_FUNCL'] = Floyd_TRANSCAD_FILE_AB_2035['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_AB_2035['AB_FUNCL'] = Floyd_HPMS_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2035['AB_FUNCL'] = HER_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_2035 = Floyd_TRANSCAD_FILE_AB_2035.merge(Floyd_HPMS_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "Floyd_July_volume_2035['AB_July_Volume'] = Floyd_July_volume_2035.AB_VOL_Fin * Floyd_July_volume_2035.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_2035 = Floyd_July_volume_2035.merge(ATR_HOURLY_VOLUME_AB_2035, on='AB_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_2035 = Floyd_July_volume_2035.merge(HER_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_2035 = Floyd_July_volume_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_2035 = Floyd_July_volume_2035.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_2035 = Floyd_July_24hr_matrix_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_2035[\"AB_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_2035 = Floyd_July_24hr_matrix_2035.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_2035 = pd.concat([Floyd_July_volume_Only_2035, Floyd_July_24hr_matrix_2035], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Floyd_July_Hourly_Volume_2035\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_2035)\n",
    "\n",
    "\n",
    "\n",
    "# Selecting needed files in Floyd BA\n",
    "Floyd_TRANSCAD_FILE_BA_2035 = Floyd_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_BA_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_BA_2035 = Floyd_HPMS_FACTORS_BA_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_BA_2035['BA_FUNCL'] = Floyd_TRANSCAD_FILE_BA_2035['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_BA_2035['BA_FUNCL'] = Floyd_HPMS_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2035['BA_FUNCL'] = HER_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_BA_2035 = Floyd_TRANSCAD_FILE_BA_2035.merge(Floyd_HPMS_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "Floyd_July_volume_BA_2035['BA_July_Volume'] = Floyd_July_volume_BA_2035.BA_VOL_Fin * Floyd_July_volume_BA_2035.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_BA_2035 = Floyd_July_volume_BA_2035.merge(ATR_HOURLY_VOLUME_BA_2035, on='BA_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_BA_2035 = Floyd_July_volume_BA_2035.merge(HER_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_BA_2035 = Floyd_July_volume_BA_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_BA_2035 = Floyd_July_volume_BA_2035.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_BA_2035 = Floyd_July_24hr_matrix_BA_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_BA_2035[\"BA_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_BA_2035 = Floyd_July_24hr_matrix_BA_2035.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_BA_2035 = pd.concat([Floyd_July_volume_Only_BA_2035, Floyd_July_24hr_matrix_BA_2035], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_BA_2035)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2035(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2035 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2035\n",
    "\n",
    "# Call the function for AB direction in Floyd\n",
    "funcl_summary_Floyd_ab_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Floyd_July_Hourly_Volume_2035, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Floyd\n",
    "funcl_summary_Floyd_ba_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Floyd_July_Hourly_Volume_BA_2035, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Floyd_ab_2035 = funcl_summary_Floyd_ab_2035.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ab_2035 = funcl_summary_Floyd_ab_2035.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Floyd_ba_2035 = funcl_summary_Floyd_ba_2035.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ba_2035 = funcl_summary_Floyd_ba_2035.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2035 = pd.merge(funcl_summary_Floyd_ab_2035, funcl_summary_Floyd_ba_2035, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2035['Total_NEW_VMT'] = combined_summary_2035['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2035['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2035['Total_VHT'] = combined_summary_2035['Total_VHT_AB'].fillna(0) + combined_summary_2035['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2035 = combined_summary_2035['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2035 = combined_summary_2035['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2035['Mean_Congested_Speed'] = (mean_speed_ab_2035 + mean_speed_ba_2035) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Floyd\n",
    "Floyd_VMT_2035 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2035['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2035['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2035['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2035['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Floyd\n",
    "#print(Floyd_VMT)\n",
    "#Floyd_VMT\n",
    "\n",
    "# Assuming Floyd_VMT is defined elsewhere in your code\n",
    "Floyd_VMT_df_2035 = pd.DataFrame(Floyd_VMT_2035)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Floyd_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Floyd_VMT_df_2035 = pd.merge(expected_Floyd_VMT_df, Floyd_VMT_df_2035, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Floyd_VMT_df_2035[\"Total_NEW_VMT\"] = final_Floyd_VMT_df_2035[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Floyd_VMT_df_2035[\"Total_VHT\"] = final_Floyd_VMT_df_2035[\"Total_VHT\"].astype(float)\n",
    "final_Floyd_VMT_df_2035[\"Mean_Congested_Speed\"] = final_Floyd_VMT_df_2035[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Floyd_VMT_df_2035[final_Floyd_VMT_df_2035['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Floyd_VMT_df_2035[final_Floyd_VMT_df_2035['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Floyd_VMT_df_2035 = pd.concat([final_Floyd_VMT_df_2035, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Floyd_VMT_df_2035['Speed'] = final_Floyd_VMT_df_2035['Total_NEW_VMT'] / final_Floyd_VMT_df_2035['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Floyd_VMT_df_2035) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2035, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2035, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2035, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2035, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Floyd_VMT_df_2035\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47d2d08e-e59e-46b2-b875-4c5ec3543fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Floyd_VMT_df_2035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d98f8b-8e70-4834-844f-9c005ef4f7bf",
   "metadata": {},
   "source": [
    "# 2035 Jefferson County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c70c2ae-307a-4f29-ad7b-769fcdc9da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3559812917.py:194: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Jefferson \n",
    "# Selecting needed files in Jefferson AB\n",
    "Jefferson_TRANSCAD_FILE_AB_2035 = Jefferson_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_AB_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_AB_2035 = Jefferson_HPMS_FACTORS_AB_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_AB_2035['AB_FUNCL'] = Jefferson_TRANSCAD_FILE_AB_2035['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_AB_2035['AB_FUNCL'] = Jefferson_HPMS_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2035['AB_FUNCL'] = HER_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_2035 = Jefferson_TRANSCAD_FILE_AB_2035.merge(Jefferson_HPMS_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "Jefferson_July_volume_2035['AB_July_Volume'] = Jefferson_July_volume_2035.AB_VOL_Fin * Jefferson_July_volume_2035.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_2035 = Jefferson_July_volume_2035.merge(ATR_HOURLY_VOLUME_AB_2035, on='AB_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_2035 = Jefferson_July_volume_2035.merge(HER_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_2035 = Jefferson_July_volume_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_2035 = Jefferson_July_volume_2035.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_2035 = Jefferson_July_24hr_matrix_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_2035[\"AB_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_2035 = Jefferson_July_24hr_matrix_2035.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2035 = pd.concat([Jefferson_July_volume_Only_2035, Jefferson_July_24hr_matrix_2035], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2035\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_2035)\n",
    "\n",
    "\n",
    "\n",
    "# Selecting needed files in Jefferson BA\n",
    "Jefferson_TRANSCAD_FILE_BA_2035 = Jefferson_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_BA_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_BA_2035 = Jefferson_HPMS_FACTORS_BA_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_BA_2035['BA_FUNCL'] = Jefferson_TRANSCAD_FILE_BA_2035['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_BA_2035['BA_FUNCL'] = Jefferson_HPMS_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2035['BA_FUNCL'] = HER_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_BA_2035 = Jefferson_TRANSCAD_FILE_BA_2035.merge(Jefferson_HPMS_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "Jefferson_July_volume_BA_2035['BA_July_Volume'] = Jefferson_July_volume_BA_2035.BA_VOL_Fin * Jefferson_July_volume_BA_2035.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_BA_2035 = Jefferson_July_volume_BA_2035.merge(ATR_HOURLY_VOLUME_BA_2035, on='BA_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_BA_2035 = Jefferson_July_volume_BA_2035.merge(HER_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_BA_2035 = Jefferson_July_volume_BA_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_BA_2035 = Jefferson_July_volume_BA_2035.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_BA_2035 = Jefferson_July_24hr_matrix_BA_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_BA_2035[\"BA_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_BA_2035 = Jefferson_July_24hr_matrix_BA_2035.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_BA_2035 = pd.concat([Jefferson_July_volume_Only_BA_2035, Jefferson_July_24hr_matrix_BA_2035], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_BA_2035)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2035(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2035 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2035\n",
    "\n",
    "# Call the function for AB direction in Jefferson\n",
    "funcl_summary_Jefferson_ab_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Jefferson_July_Hourly_Volume_2035, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Jefferson\n",
    "funcl_summary_Jefferson_ba_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Jefferson_July_Hourly_Volume_BA_2035, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Jefferson_ab_2035 = funcl_summary_Jefferson_ab_2035.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ab_2035 = funcl_summary_Jefferson_ab_2035.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Jefferson_ba_2035 = funcl_summary_Jefferson_ba_2035.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ba_2035 = funcl_summary_Jefferson_ba_2035.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2035 = pd.merge(funcl_summary_Jefferson_ab_2035, funcl_summary_Jefferson_ba_2035, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2035['Total_NEW_VMT'] = combined_summary_2035['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2035['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2035['Total_VHT'] = combined_summary_2035['Total_VHT_AB'].fillna(0) + combined_summary_2035['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2035 = combined_summary_2035['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2035 = combined_summary_2035['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2035['Mean_Congested_Speed'] = (mean_speed_ab_2035 + mean_speed_ba_2035) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Jefferson\n",
    "Jefferson_VMT_2035 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2035['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2035['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2035['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2035['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Jefferson\n",
    "#print(Jefferson_VMT)\n",
    "#Jefferson_VMT\n",
    "\n",
    "# Assuming Jefferson_VMT is defined elsewhere in your code\n",
    "Jefferson_VMT_df_2035 = pd.DataFrame(Jefferson_VMT_2035)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Jefferson_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Jefferson_VMT_df_2035 = pd.merge(expected_Jefferson_VMT_df, Jefferson_VMT_df_2035, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Jefferson_VMT_df_2035[\"Total_NEW_VMT\"] = final_Jefferson_VMT_df_2035[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2035[\"Total_VHT\"] = final_Jefferson_VMT_df_2035[\"Total_VHT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2035[\"Mean_Congested_Speed\"] = final_Jefferson_VMT_df_2035[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Jefferson_VMT_df_2035[final_Jefferson_VMT_df_2035['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Jefferson_VMT_df_2035[final_Jefferson_VMT_df_2035['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Jefferson_VMT_df_2035 = pd.concat([final_Jefferson_VMT_df_2035, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Jefferson_VMT_df_2035['Speed'] = final_Jefferson_VMT_df_2035['Total_NEW_VMT'] / final_Jefferson_VMT_df_2035['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Jefferson_VMT_df_2035) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2035, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2035, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2035, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2035, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Jefferson_VMT_df_2035\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b01a3e2-8309-4d9f-b3b2-94200dd66ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Jefferson_VMT_df_2035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e659307-a616-4c91-8cf6-9da749a4da4e",
   "metadata": {},
   "source": [
    "# 2035 Oldam County Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8acb0293-fad6-41a5-b76b-2e655e085584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\893513868.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Oldham \n",
    "# Selecting needed files in Oldham AB\n",
    "Oldham_TRANSCAD_FILE_AB_2035 = Oldham_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_AB_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_AB_2035 = Oldham_HPMS_FACTORS_AB_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_AB_2035['AB_FUNCL'] = Oldham_TRANSCAD_FILE_AB_2035['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2035['AB_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_AB_2035['AB_FUNCL'] = Oldham_HPMS_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2035['AB_FUNCL'] = HER_FACTORS_AB_2035['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_2035 = Oldham_TRANSCAD_FILE_AB_2035.merge(Oldham_HPMS_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "Oldham_July_volume_2035['AB_July_Volume'] = Oldham_July_volume_2035.AB_VOL_Fin * Oldham_July_volume_2035.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_2035 = Oldham_July_volume_2035.merge(ATR_HOURLY_VOLUME_AB_2035, on='AB_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_2035 = Oldham_July_volume_2035.merge(HER_FACTORS_AB_2035, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_2035 = Oldham_July_volume_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_2035 = Oldham_July_volume_2035.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_2035 = Oldham_July_24hr_matrix_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_2035[\"AB_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_2035 = Oldham_July_24hr_matrix_2035.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_2035 = pd.concat([Oldham_July_volume_Only_2035, Oldham_July_24hr_matrix_2035], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Oldham_July_Hourly_Volume_2035\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_2035)\n",
    "\n",
    "\n",
    "\n",
    "# Selecting needed files in Oldham BA\n",
    "Oldham_TRANSCAD_FILE_BA_2035 = Oldham_TRANSCAD_FILE_2035.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2035 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_BA_2035 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_BA_2035 = Oldham_HPMS_FACTORS_BA_2035.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2035 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_BA_2035['BA_FUNCL'] = Oldham_TRANSCAD_FILE_BA_2035['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2035['BA_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_BA_2035['BA_FUNCL'] = Oldham_HPMS_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2035['BA_FUNCL'] = HER_FACTORS_BA_2035['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_BA_2035 = Oldham_TRANSCAD_FILE_BA_2035.merge(Oldham_HPMS_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "Oldham_July_volume_BA_2035['BA_July_Volume'] = Oldham_July_volume_BA_2035.BA_VOL_Fin * Oldham_July_volume_BA_2035.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_BA_2035 = Oldham_July_volume_BA_2035.merge(ATR_HOURLY_VOLUME_BA_2035, on='BA_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_BA_2035 = Oldham_July_volume_BA_2035.merge(HER_FACTORS_BA_2035, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_BA_2035 = Oldham_July_volume_BA_2035.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_BA_2035 = Oldham_July_volume_BA_2035.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_BA_2035 = Oldham_July_24hr_matrix_BA_2035[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_BA_2035[\"BA_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_BA_2035 = Oldham_July_24hr_matrix_BA_2035.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_BA_2035 = pd.concat([Oldham_July_volume_Only_BA_2035, Oldham_July_24hr_matrix_BA_2035], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_BA_2035)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2035(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2035 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2035 = pd.concat([summary_df_2035, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2035\n",
    "\n",
    "# Call the function for AB direction in Oldham\n",
    "funcl_summary_Oldham_ab_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Oldham_July_Hourly_Volume_2035, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Oldham\n",
    "funcl_summary_Oldham_ba_2035 = sum_new_vmt_vht_and_speed_by_direction_2035(Oldham_July_Hourly_Volume_BA_2035, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Oldham_ab_2035 = funcl_summary_Oldham_ab_2035.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ab_2035 = funcl_summary_Oldham_ab_2035.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Oldham_ba_2035 = funcl_summary_Oldham_ba_2035.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ba_2035 = funcl_summary_Oldham_ba_2035.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2035 = pd.merge(funcl_summary_Oldham_ab_2035, funcl_summary_Oldham_ba_2035, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2035['Total_NEW_VMT'] = combined_summary_2035['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2035['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2035['Total_VHT'] = combined_summary_2035['Total_VHT_AB'].fillna(0) + combined_summary_2035['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2035 = combined_summary_2035['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2035 = combined_summary_2035['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2035['Mean_Congested_Speed'] = (mean_speed_ab_2035 + mean_speed_ba_2035) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Oldham\n",
    "Oldham_VMT_2035 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2035['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2035['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2035['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2035['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Oldham\n",
    "#print(Oldham_VMT)\n",
    "#Oldham_VMT\n",
    "\n",
    "# Assuming Oldham_VMT is defined elsewhere in your code\n",
    "Oldham_VMT_df_2035 = pd.DataFrame(Oldham_VMT_2035)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Oldham_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Oldham_VMT_df_2035 = pd.merge(expected_Oldham_VMT_df, Oldham_VMT_df_2035, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Oldham_VMT_df_2035[\"Total_NEW_VMT\"] = final_Oldham_VMT_df_2035[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Oldham_VMT_df_2035[\"Total_VHT\"] = final_Oldham_VMT_df_2035[\"Total_VHT\"].astype(float)\n",
    "final_Oldham_VMT_df_2035[\"Mean_Congested_Speed\"] = final_Oldham_VMT_df_2035[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Oldham_VMT_df_2035[final_Oldham_VMT_df_2035['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Oldham_VMT_df_2035[final_Oldham_VMT_df_2035['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Oldham_VMT_df_2035 = pd.concat([final_Oldham_VMT_df_2035, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Oldham_VMT_df_2035['Speed'] = final_Oldham_VMT_df_2035['Total_NEW_VMT'] / final_Oldham_VMT_df_2035['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Oldham_VMT_df_2035) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2035, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2035, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2035, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2035 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2035, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Oldham_VMT_df_2035\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5889f3c-bf6d-41d6-9484-7efee00b939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Oldham_VMT_df_2035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0343d9-0928-41d7-a926-1d0a072b014f",
   "metadata": {},
   "source": [
    "# Combine 2035 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f77a7ae-c540-42a0-b422-3faef9dc0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Create a blank DataFrame with the same columns to use for spacing\n",
    "blank_lines = pd.DataFrame(np.nan, index=range(5), columns=Clark_VMT.columns)\n",
    "\n",
    "# Initialize a list to hold the DataFrames and their titles\n",
    "dataframes_with_titles = []\n",
    "\n",
    "# Append each DataFrame with its title and blank lines\n",
    "for county, df in zip(['Bullit', 'Clark', 'Floyd', 'Jefferson', 'Oldham'], \n",
    "                      [final_Bullit_VMT_df_2035, final_Clark_VMT_df_2035, final_Floyd_VMT_df_2035, final_Jefferson_VMT_df_2035, final_Oldham_VMT_df_2035]):\n",
    "    # Create a DataFrame for the title (bold)\n",
    "    title_df = pd.DataFrame(columns=df.columns)\n",
    "    title_df.loc[0] = df.columns  # Set the title row\n",
    "    # Append the county name with \"County\" as the title\n",
    "    dataframes_with_titles.append((f\"{county} County\", title_df, df, blank_lines))\n",
    "\n",
    "# Create a new Excel workbook\n",
    "output_file_path = '2035_Combined_VMT_Data.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = 'VMT_Data'\n",
    "\n",
    "# Add the dataframes and format them\n",
    "row_offset = 0\n",
    "for title, title_df, df, blanks in dataframes_with_titles:\n",
    "    # Add the file name with \"County\" as a subheading above each DataFrame\n",
    "    ws.cell(row=row_offset + 1, column=1, value=title).font = Font(bold=True)\n",
    "    row_offset += 1  # Move to the next row for the title\n",
    "\n",
    "    # Write the column titles (bold)\n",
    "    for c_idx, column_name in enumerate(title_df.columns, start=1):\n",
    "        ws.cell(row=row_offset + 1, column=c_idx, value=column_name).font = Font(bold=True)\n",
    "\n",
    "    # Write the DataFrame data\n",
    "    for r_idx, row in enumerate(df.values, start=row_offset + 2):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Adjust column widths based on the maximum length of data in each column\n",
    "    for c_idx in range(1, df.shape[1] + 1):\n",
    "        max_length = 0\n",
    "        # Check title length\n",
    "        max_length = max(max_length, len(title_df.columns[c_idx - 1]))  \n",
    "        # Check data length\n",
    "        for r_idx in range(df.shape[0]):\n",
    "            cell_value = str(df.iat[r_idx, c_idx - 1])  # Get cell value\n",
    "            max_length = max(max_length, len(cell_value))  # Update max length if necessary\n",
    "        ws.column_dimensions[ws.cell(row=1, column=c_idx).column_letter].width = max_length + 2  # Add some padding\n",
    "\n",
    "    # Update the row_offset for the next DataFrame\n",
    "    row_offset += df.shape[0] + 3  # 2 for the data and 1 for the title\n",
    "\n",
    "    # Add blank lines\n",
    "    for _ in range(5):\n",
    "        row_offset += 1  # Add blank lines between DataFrames\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9515517-3967-4b47-ba85-b97b5b629304",
   "metadata": {},
   "source": [
    "# 2040 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "055ff747-8f59-4674-b053-ead37254e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant variables\n",
    "TRANSCAD_RESULT_FILE_2040 = TRANSCAD_RESULT_FILE_2040.loc[:, ['ID', 'Length', 'Link_AADT', \n",
    "                                             'County', 'AB_FACT', 'FUNCL', 'BA_FACT', \n",
    "                                              'AB_FFSPD', 'BA_FFSPD', 'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                              'BA_VOL_Fin', 'Tot_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "\n",
    "# Fill null values in Link_AADT and convert types\n",
    "TRANSCAD_RESULT_FILE_2040['Link_AADT'] = TRANSCAD_RESULT_FILE_2040['Link_AADT'].fillna(0)\n",
    "TRANSCAD_RESULT_FILE_2040[\"Link_AADT\"] = TRANSCAD_RESULT_FILE_2040[\"Link_AADT\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"AB_CAPDAY\"] = TRANSCAD_RESULT_FILE_2040[\"AB_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"BA_CAPDAY\"] = TRANSCAD_RESULT_FILE_2040[\"BA_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"FUNCL\"] = TRANSCAD_RESULT_FILE_2040[\"FUNCL\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"AB_FFSPD\"] = TRANSCAD_RESULT_FILE_2040[\"AB_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"BA_FFSPD\"] = TRANSCAD_RESULT_FILE_2040[\"BA_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"B_exponent\"] = TRANSCAD_RESULT_FILE_2040[\"B_exponent\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2040[\"A_coefficient\"] = TRANSCAD_RESULT_FILE_2040[\"A_coefficient\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2040[\"BA_VOL_Fin\"] = TRANSCAD_RESULT_FILE_2040[\"BA_VOL_Fin\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2040[\"Length\"] = TRANSCAD_RESULT_FILE_2040[\"Length\"].astype(float)\n",
    "\n",
    "\n",
    "# Calculate Peak Hour Capacity\n",
    "TRANSCAD_RESULT_FILE_2040['AB_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2040.AB_CAPDAY / 10\n",
    "TRANSCAD_RESULT_FILE_2040['BA_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2040.BA_CAPDAY / 10\n",
    "\n",
    "# Create duplicate points column\n",
    "TRANSCAD_RESULT_FILE_2040['AB_FUNCL'] = TRANSCAD_RESULT_FILE_2040.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2040['BA_FUNCL'] = TRANSCAD_RESULT_FILE_2040.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2040['AB_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2040.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2040['BA_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2040.loc[:, 'FUNCL']\n",
    "\n",
    "#Library original\n",
    "TRANSCAD_RESULT_FILE_2040[\"AB_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                   8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                   18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "#Library\n",
    "TRANSCAD_RESULT_FILE_2040[\"BA_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                  8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                  18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2040[\"AB_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2040[\"BA_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ba2afe7-7299-462f-a7bc-33013338dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating different county files \n",
    "Bullit_TRANSCAD_FILE_2040 = TRANSCAD_RESULT_FILE_2040.loc[TRANSCAD_RESULT_FILE_2040.County == 'Bullitt', :]\n",
    "\n",
    "Clark_TRANSCAD_FILE_2040 = TRANSCAD_RESULT_FILE_2040.loc[TRANSCAD_RESULT_FILE_2040.County == 'Clark', :]\n",
    "\n",
    "Floyd_TRANSCAD_FILE_2040 = TRANSCAD_RESULT_FILE_2040.loc[TRANSCAD_RESULT_FILE_2040.County == 'Floyd', :]\n",
    "\n",
    "Jefferson_TRANSCAD_FILE_2040 = TRANSCAD_RESULT_FILE_2040.loc[TRANSCAD_RESULT_FILE_2040.County == 'Jefferson', :]\n",
    "\n",
    "Oldham_TRANSCAD_FILE_2040 = TRANSCAD_RESULT_FILE_2040.loc[TRANSCAD_RESULT_FILE_2040.County == 'Oldham', :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf997f-d273-4267-b0ea-cb97c9910290",
   "metadata": {},
   "source": [
    "# 2040 Bullit Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0361669d-a122-4256-aab1-5c5983ef10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\652573129.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Bullit \n",
    "# Selecting needed files in Bullit AB\n",
    "Bullit_TRANSCAD_FILE_AB_2040 = Bullit_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_AB_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_AB_2040 = Bullit_HPMS_FACTORS_AB_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_AB_2040['AB_FUNCL'] = Bullit_TRANSCAD_FILE_AB_2040['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_AB_2040['AB_FUNCL'] = Bullit_HPMS_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2040['AB_FUNCL'] = HER_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume_2040 = Bullit_TRANSCAD_FILE_AB_2040.merge(Bullit_HPMS_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "Bullit_July_volume_2040['AB_July_Volume'] = Bullit_July_volume_2040.AB_VOL_Fin * Bullit_July_volume_2040.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_2040 = Bullit_July_volume_2040.merge(ATR_HOURLY_VOLUME_AB_2040, on='AB_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_2040 = Bullit_July_volume_2040.merge(HER_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_2040 = Bullit_July_volume_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_2040 = Bullit_July_volume_2040.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_2040 = Bullit_July_24hr_matrix_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_2040[\"AB_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_2040 = Bullit_July_24hr_matrix_2040.fillna(0)\n",
    "\n",
    "Bullit_July_Hourly_Volume_2040 = pd.concat([Bullit_July_volume_Only_2040, Bullit_July_24hr_matrix_2040], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_2040\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_2040)\n",
    "\n",
    "\n",
    "\n",
    "# Selecting needed files in Bullit BA\n",
    "Bullit_TRANSCAD_FILE_BA_2040 = Bullit_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_BA_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_BA_2040 = Bullit_HPMS_FACTORS_BA_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_BA_2040['BA_FUNCL'] = Bullit_TRANSCAD_FILE_BA_2040['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_BA_2040['BA_FUNCL'] = Bullit_HPMS_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2040['BA_FUNCL'] = HER_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume_BA_2040 = Bullit_TRANSCAD_FILE_BA_2040.merge(Bullit_HPMS_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "Bullit_July_volume_BA_2040['BA_July_Volume'] = Bullit_July_volume_BA_2040.BA_VOL_Fin * Bullit_July_volume_BA_2040.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_BA_2040 = Bullit_July_volume_BA_2040.merge(ATR_HOURLY_VOLUME_BA_2040, on='BA_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_BA_2040 = Bullit_July_volume_BA_2040.merge(HER_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_BA_2040 = Bullit_July_volume_BA_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_BA_2040 = Bullit_July_volume_BA_2040.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_BA_2040 = Bullit_July_24hr_matrix_BA_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_BA_2040[\"BA_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_BA_2040 = Bullit_July_24hr_matrix_BA_2040.fillna(0)\n",
    "\n",
    "Bullit_July_Hourly_Volume_BA_2040 = pd.concat([Bullit_July_volume_Only_BA_2040, Bullit_July_24hr_matrix_BA_2040], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_BA_2040)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2040(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2040 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2040\n",
    "\n",
    "# Call the function for AB direction in Bullit\n",
    "funcl_summary_Bullit_ab_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Bullit_July_Hourly_Volume_2040, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Bullit\n",
    "funcl_summary_Bullit_ba_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Bullit_July_Hourly_Volume_BA_2040, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Bullit_ab_2040 = funcl_summary_Bullit_ab_2040.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ab_2040 = funcl_summary_Bullit_ab_2040.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Bullit_ba_2040 = funcl_summary_Bullit_ba_2040.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ba_2040 = funcl_summary_Bullit_ba_2040.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2040 = pd.merge(funcl_summary_Bullit_ab_2040, funcl_summary_Bullit_ba_2040, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2040['Total_NEW_VMT'] = combined_summary_2040['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2040['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2040['Total_VHT'] = combined_summary_2040['Total_VHT_AB'].fillna(0) + combined_summary_2040['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2040 = combined_summary_2040['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2040 = combined_summary_2040['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2040['Mean_Congested_Speed'] = (mean_speed_ab_2040 + mean_speed_ba_2040) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Bullit\n",
    "Bullit_VMT_2040 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2040['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2040['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2040['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2040['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Bullit\n",
    "#print(Bullit_VMT)\n",
    "#Bullit_VMT\n",
    "\n",
    "# Assuming Bullit_VMT is defined elsewhere in your code\n",
    "Bullit_VMT_df_2040 = pd.DataFrame(Bullit_VMT_2040)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Bullit_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Bullit_VMT_df_2040 = pd.merge(expected_Bullit_VMT_df, Bullit_VMT_df_2040, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Bullit_VMT_df_2040[\"Total_NEW_VMT\"] = final_Bullit_VMT_df_2040[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Bullit_VMT_df_2040[\"Total_VHT\"] = final_Bullit_VMT_df_2040[\"Total_VHT\"].astype(float)\n",
    "final_Bullit_VMT_df_2040[\"Mean_Congested_Speed\"] = final_Bullit_VMT_df_2040[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Bullit_VMT_df_2040[final_Bullit_VMT_df_2040['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Bullit_VMT_df_2040[final_Bullit_VMT_df_2040['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Bullit_VMT_df_2040 = pd.concat([final_Bullit_VMT_df_2040, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Bullit_VMT_df_2040['Speed'] = final_Bullit_VMT_df_2040['Total_NEW_VMT'] / final_Bullit_VMT_df_2040['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Bullit_VMT_df_2040) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2040, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2040, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2040, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2040, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Bullit_VMT_df_2040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a7d145f-3578-42bc-814d-7f24f456332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Bullit_VMT_df_2040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4d099-a398-42bd-8924-fa7114ed5cf9",
   "metadata": {},
   "source": [
    "# 2040 Clark Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "879a18c2-72c2-4f62-8881-75ac65dba747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\3027844325.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Clark \n",
    "# Selecting needed files in Clark AB\n",
    "Clark_TRANSCAD_FILE_AB_2040 = Clark_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_AB_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_AB_2040 = Clark_HPMS_FACTORS_AB_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_AB_2040['AB_FUNCL'] = Clark_TRANSCAD_FILE_AB_2040['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_AB_2040['AB_FUNCL'] = Clark_HPMS_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2040['AB_FUNCL'] = HER_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_2040 = Clark_TRANSCAD_FILE_AB_2040.merge(Clark_HPMS_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "Clark_July_volume_2040['AB_July_Volume'] = Clark_July_volume_2040.AB_VOL_Fin * Clark_July_volume_2040.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_2040 = Clark_July_volume_2040.merge(ATR_HOURLY_VOLUME_AB_2040, on='AB_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_2040 = Clark_July_volume_2040.merge(HER_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_2040 = Clark_July_volume_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_2040 = Clark_July_volume_2040.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_2040 = Clark_July_24hr_matrix_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_2040[\"AB_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_2040 = Clark_July_24hr_matrix_2040.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_2040 = pd.concat([Clark_July_volume_Only_2040, Clark_July_24hr_matrix_2040], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Clark_July_Hourly_Volume_2040\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_2040)\n",
    "\n",
    "\n",
    "# Selecting needed files in Clark BA\n",
    "Clark_TRANSCAD_FILE_BA_2040 = Clark_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_BA_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_BA_2040 = Clark_HPMS_FACTORS_BA_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_BA_2040['BA_FUNCL'] = Clark_TRANSCAD_FILE_BA_2040['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_BA_2040['BA_FUNCL'] = Clark_HPMS_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2040['BA_FUNCL'] = HER_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_BA_2040 = Clark_TRANSCAD_FILE_BA_2040.merge(Clark_HPMS_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "Clark_July_volume_BA_2040['BA_July_Volume'] = Clark_July_volume_BA_2040.BA_VOL_Fin * Clark_July_volume_BA_2040.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_BA_2040 = Clark_July_volume_BA_2040.merge(ATR_HOURLY_VOLUME_BA_2040, on='BA_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_BA_2040 = Clark_July_volume_BA_2040.merge(HER_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_BA_2040 = Clark_July_volume_BA_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_BA_2040 = Clark_July_volume_BA_2040.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_BA_2040 = Clark_July_24hr_matrix_BA_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_BA_2040[\"BA_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_BA_2040 = Clark_July_24hr_matrix_BA_2040.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_BA_2040 = pd.concat([Clark_July_volume_Only_BA_2040, Clark_July_24hr_matrix_BA_2040], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_BA_2040)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2040(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2040 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2040\n",
    "\n",
    "# Call the function for AB direction in Clark\n",
    "funcl_summary_Clark_ab_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Clark_July_Hourly_Volume_2040, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Clark\n",
    "funcl_summary_Clark_ba_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Clark_July_Hourly_Volume_BA_2040, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Clark_ab_2040 = funcl_summary_Clark_ab_2040.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ab_2040 = funcl_summary_Clark_ab_2040.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Clark_ba_2040 = funcl_summary_Clark_ba_2040.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ba_2040 = funcl_summary_Clark_ba_2040.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2040 = pd.merge(funcl_summary_Clark_ab_2040, funcl_summary_Clark_ba_2040, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2040['Total_NEW_VMT'] = combined_summary_2040['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2040['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2040['Total_VHT'] = combined_summary_2040['Total_VHT_AB'].fillna(0) + combined_summary_2040['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2040 = combined_summary_2040['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2040 = combined_summary_2040['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2040['Mean_Congested_Speed'] = (mean_speed_ab_2040 + mean_speed_ba_2040) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Clark\n",
    "Clark_VMT_2040 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2040['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2040['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2040['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2040['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Clark\n",
    "#print(Clark_VMT)\n",
    "#Clark_VMT\n",
    "\n",
    "# Assuming Clark_VMT is defined elsewhere in your code\n",
    "Clark_VMT_df_2040 = pd.DataFrame(Clark_VMT_2040)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Clark_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Clark_VMT_df_2040 = pd.merge(expected_Clark_VMT_df, Clark_VMT_df_2040, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Clark_VMT_df_2040[\"Total_NEW_VMT\"] = final_Clark_VMT_df_2040[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Clark_VMT_df_2040[\"Total_VHT\"] = final_Clark_VMT_df_2040[\"Total_VHT\"].astype(float)\n",
    "final_Clark_VMT_df_2040[\"Mean_Congested_Speed\"] = final_Clark_VMT_df_2040[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Clark_VMT_df_2040[final_Clark_VMT_df_2040['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Clark_VMT_df_2040[final_Clark_VMT_df_2040['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Clark_VMT_df_2040 = pd.concat([final_Clark_VMT_df_2040, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Clark_VMT_df_2040['Speed'] = final_Clark_VMT_df_2040['Total_NEW_VMT'] / final_Clark_VMT_df_2040['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Clark_VMT_df_2040) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2040, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2040, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2040, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2040, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Clark_VMT_df_2040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c282a88-716b-47ce-a37e-cd3bab8729ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Clark_VMT_df_2040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0a293-ba81-41ae-88e8-925af50dcd2b",
   "metadata": {},
   "source": [
    "# 2040 Floyd Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05f81555-4be0-4ed9-8a52-6d20fc348aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:339: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] +\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:351: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\875442735.py:352: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n"
     ]
    }
   ],
   "source": [
    "# Floyd \n",
    "# Selecting needed files in Floyd AB\n",
    "Floyd_TRANSCAD_FILE_AB_2040 = Floyd_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_AB_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_AB_2040 = Floyd_HPMS_FACTORS_AB_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_AB_2040['AB_FUNCL'] = Floyd_TRANSCAD_FILE_AB_2040['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_AB_2040['AB_FUNCL'] = Floyd_HPMS_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2040['AB_FUNCL'] = HER_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_2040 = Floyd_TRANSCAD_FILE_AB_2040.merge(Floyd_HPMS_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "Floyd_July_volume_2040['AB_July_Volume'] = Floyd_July_volume_2040.AB_VOL_Fin * Floyd_July_volume_2040.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_2040 = Floyd_July_volume_2040.merge(ATR_HOURLY_VOLUME_AB_2040, on='AB_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_2040 = Floyd_July_volume_2040.merge(HER_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_2040 = Floyd_July_volume_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                             \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_2040 = Floyd_July_volume_2040.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_2040 = Floyd_July_24hr_matrix_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_2040[\"AB_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_2040 = Floyd_July_24hr_matrix_2040.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_2040 = pd.concat([Floyd_July_volume_Only_2040, Floyd_July_24hr_matrix_2040], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Floyd_July_Hourly_Volume_2040\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_2040)\n",
    "\n",
    "\n",
    "# Selecting needed files in Floyd BA\n",
    "Floyd_TRANSCAD_FILE_BA_2040 = Floyd_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                               'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                               'BA_CAPDAY', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_BA_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_BA_2040 = Floyd_HPMS_FACTORS_BA_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_BA_2040['BA_FUNCL'] = Floyd_TRANSCAD_FILE_BA_2040['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_BA_2040['BA_FUNCL'] = Floyd_HPMS_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2040['BA_FUNCL'] = HER_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_BA_2040 = Floyd_TRANSCAD_FILE_BA_2040.merge(Floyd_HPMS_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "Floyd_July_volume_BA_2040['BA_July_Volume'] = Floyd_July_volume_BA_2040.BA_VOL_Fin * Floyd_July_volume_BA_2040.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_BA_2040 = Floyd_July_volume_BA_2040.merge(ATR_HOURLY_VOLUME_BA_2040, on='BA_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_BA_2040 = Floyd_July_volume_BA_2040.merge(HER_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_BA_2040 = Floyd_July_volume_BA_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                 \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_BA_2040 = Floyd_July_volume_BA_2040.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_BA_2040 = Floyd_July_24hr_matrix_BA_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                  'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                  'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                  'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                  'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                  'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_BA_2040[\"BA_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_BA_2040 = Floyd_July_24hr_matrix_BA_2040.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_BA_2040 = pd.concat([Floyd_July_volume_Only_BA_2040, Floyd_July_24hr_matrix_BA_2040], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_BA_2040)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2040(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2040 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2040\n",
    "\n",
    "# Call the function for AB direction in Floyd\n",
    "funcl_summary_Floyd_ab_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Floyd_July_Hourly_Volume_2040, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Floyd\n",
    "funcl_summary_Floyd_ba_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Floyd_July_Hourly_Volume_BA_2040, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Floyd_ab_2040 = funcl_summary_Floyd_ab_2040.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ab_2040 = funcl_summary_Floyd_ab_2040.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Floyd_ba_2040 = funcl_summary_Floyd_ba_2040.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ba_2040 = funcl_summary_Floyd_ba_2040.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2040 = pd.merge(funcl_summary_Floyd_ab_2040, funcl_summary_Floyd_ba_2040, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2040['Total_NEW_VMT'] = combined_summary_2040['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2040['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2040['Total_VHT'] = combined_summary_2040['Total_VHT_AB'].fillna(0) + combined_summary_2040['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2040 = combined_summary_2040['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2040 = combined_summary_2040['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2040['Mean_Congested_Speed'] = (mean_speed_ab_2040 + mean_speed_ba_2040) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Floyd\n",
    "Floyd_VMT_2040 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2040['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2040['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2040['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2040['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Floyd\n",
    "#print(Floyd_VMT)\n",
    "#Floyd_VMT\n",
    "\n",
    "# Assuming Floyd_VMT is defined elsewhere in your code\n",
    "Floyd_VMT_df_2040 = pd.DataFrame(Floyd_VMT_2040)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Floyd_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Floyd_VMT_df_2040 = pd.merge(expected_Floyd_VMT_df, Floyd_VMT_df_2040, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Floyd_VMT_df_2040[\"Total_NEW_VMT\"] = final_Floyd_VMT_df_2040[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Floyd_VMT_df_2040[\"Total_VHT\"] = final_Floyd_VMT_df_2040[\"Total_VHT\"].astype(float)\n",
    "final_Floyd_VMT_df_2040[\"Mean_Congested_Speed\"] = final_Floyd_VMT_df_2040[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Floyd_VMT_df_2040[final_Floyd_VMT_df_2040['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Floyd_VMT_df_2040[final_Floyd_VMT_df_2040['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Floyd_VMT_df_2040 = pd.concat([final_Floyd_VMT_df_2040, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Floyd_VMT_df_2040['Speed'] = final_Floyd_VMT_df_2040['Total_NEW_VMT'] / final_Floyd_VMT_df_2040['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Floyd_VMT_df_2040) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2040, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2040, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2040, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2040, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Floyd_VMT_df_2040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18645965-3904-488b-a173-eca75ba689b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Floyd_VMT_df_2040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c466d92-b687-4050-8ef7-4eb4984cf840",
   "metadata": {},
   "source": [
    "# 2040 Jefferson Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bee216b5-aeb6-411b-a16d-f3e2dd49d14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1479015922.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Jefferson \n",
    "# Selecting needed files in Jefferson AB\n",
    "Jefferson_TRANSCAD_FILE_AB_2040 = Jefferson_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                      'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                      'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                      'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_AB_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_AB_2040 = Jefferson_HPMS_FACTORS_AB_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_AB_2040['AB_FUNCL'] = Jefferson_TRANSCAD_FILE_AB_2040['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_AB_2040['AB_FUNCL'] = Jefferson_HPMS_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2040['AB_FUNCL'] = HER_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_2040 = Jefferson_TRANSCAD_FILE_AB_2040.merge(Jefferson_HPMS_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "Jefferson_July_volume_2040['AB_July_Volume'] = Jefferson_July_volume_2040.AB_VOL_Fin * Jefferson_July_volume_2040.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_2040 = Jefferson_July_volume_2040.merge(ATR_HOURLY_VOLUME_AB_2040, on='AB_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_2040 = Jefferson_July_volume_2040.merge(HER_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_2040 = Jefferson_July_volume_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                     \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                     \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_2040 = Jefferson_July_volume_2040.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                     'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                     'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                     'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                     'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                     'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                     'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_2040 = Jefferson_July_24hr_matrix_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                    'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                    'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                    'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                    'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                    'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_2040[\"AB_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_2040 = Jefferson_July_24hr_matrix_2040.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2040 = pd.concat([Jefferson_July_volume_Only_2040, Jefferson_July_24hr_matrix_2040], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2040\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_2040)\n",
    "\n",
    "\n",
    "# Selecting needed files in Jefferson BA\n",
    "Jefferson_TRANSCAD_FILE_BA_2040 = Jefferson_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                       'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                       'BA_CAPDAY', \n",
    "                                                                       'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_BA_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_BA_2040 = Jefferson_HPMS_FACTORS_BA_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_BA_2040['BA_FUNCL'] = Jefferson_TRANSCAD_FILE_BA_2040['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_BA_2040['BA_FUNCL'] = Jefferson_HPMS_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2040['BA_FUNCL'] = HER_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_BA_2040 = Jefferson_TRANSCAD_FILE_BA_2040.merge(Jefferson_HPMS_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "Jefferson_July_volume_BA_2040['BA_July_Volume'] = Jefferson_July_volume_BA_2040.BA_VOL_Fin * Jefferson_July_volume_BA_2040.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_BA_2040 = Jefferson_July_volume_BA_2040.merge(ATR_HOURLY_VOLUME_BA_2040, on='BA_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_BA_2040 = Jefferson_July_volume_BA_2040.merge(HER_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_BA_2040 = Jefferson_July_volume_BA_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                         \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                         \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_BA_2040 = Jefferson_July_volume_BA_2040.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                         'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                         'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                         'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                         'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                         'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_BA_2040 = Jefferson_July_24hr_matrix_BA_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                          'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                          'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                          'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                          'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                          'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_BA_2040[\"BA_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_BA_2040 = Jefferson_July_24hr_matrix_BA_2040.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_BA_2040 = pd.concat([Jefferson_July_volume_Only_BA_2040, Jefferson_July_24hr_matrix_BA_2040], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_BA_2040)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2040(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2040 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2040\n",
    "\n",
    "# Call the function for AB direction in Jefferson\n",
    "funcl_summary_Jefferson_ab_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Jefferson_July_Hourly_Volume_2040, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Jefferson\n",
    "funcl_summary_Jefferson_ba_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Jefferson_July_Hourly_Volume_BA_2040, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Jefferson_ab_2040 = funcl_summary_Jefferson_ab_2040.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ab_2040 = funcl_summary_Jefferson_ab_2040.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Jefferson_ba_2040 = funcl_summary_Jefferson_ba_2040.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ba_2040 = funcl_summary_Jefferson_ba_2040.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2040 = pd.merge(funcl_summary_Jefferson_ab_2040, funcl_summary_Jefferson_ba_2040, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2040['Total_NEW_VMT'] = combined_summary_2040['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2040['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2040['Total_VHT'] = combined_summary_2040['Total_VHT_AB'].fillna(0) + combined_summary_2040['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2040 = combined_summary_2040['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2040 = combined_summary_2040['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2040['Mean_Congested_Speed'] = (mean_speed_ab_2040 + mean_speed_ba_2040) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Jefferson\n",
    "Jefferson_VMT_2040 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2040['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2040['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2040['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2040['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Jefferson\n",
    "#print(Jefferson_VMT)\n",
    "#Jefferson_VMT\n",
    "\n",
    "# Assuming Jefferson_VMT is defined elsewhere in your code\n",
    "Jefferson_VMT_df_2040 = pd.DataFrame(Jefferson_VMT_2040)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Jefferson_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Jefferson_VMT_df_2040 = pd.merge(expected_Jefferson_VMT_df, Jefferson_VMT_df_2040, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Jefferson_VMT_df_2040[\"Total_NEW_VMT\"] = final_Jefferson_VMT_df_2040[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2040[\"Total_VHT\"] = final_Jefferson_VMT_df_2040[\"Total_VHT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2040[\"Mean_Congested_Speed\"] = final_Jefferson_VMT_df_2040[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Jefferson_VMT_df_2040[final_Jefferson_VMT_df_2040['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Jefferson_VMT_df_2040[final_Jefferson_VMT_df_2040['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Jefferson_VMT_df_2040 = pd.concat([final_Jefferson_VMT_df_2040, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Jefferson_VMT_df_2040['Speed'] = final_Jefferson_VMT_df_2040['Total_NEW_VMT'] / final_Jefferson_VMT_df_2040['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Jefferson_VMT_df_2040) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2040, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2040, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2040, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2040, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Jefferson_VMT_df_2040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7e447b8-7e50-4f40-bed1-35d83411c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Jefferson_VMT_df_2040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc28405-7c03-418e-be60-2e67e11f8326",
   "metadata": {},
   "source": [
    "# 2040 Oldham Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce0d3058-3c18-4e0c-92f1-157391d88b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2543440132.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Oldham \n",
    "# Selecting needed files in Oldham AB\n",
    "Oldham_TRANSCAD_FILE_AB_2040 = Oldham_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                  'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                  'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_AB_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_AB_2040 = Oldham_HPMS_FACTORS_AB_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_AB_2040['AB_FUNCL'] = Oldham_TRANSCAD_FILE_AB_2040['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2040['AB_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_AB_2040['AB_FUNCL'] = Oldham_HPMS_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2040['AB_FUNCL'] = HER_FACTORS_AB_2040['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_2040 = Oldham_TRANSCAD_FILE_AB_2040.merge(Oldham_HPMS_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "Oldham_July_volume_2040['AB_July_Volume'] = Oldham_July_volume_2040.AB_VOL_Fin * Oldham_July_volume_2040.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_2040 = Oldham_July_volume_2040.merge(ATR_HOURLY_VOLUME_AB_2040, on='AB_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_2040 = Oldham_July_volume_2040.merge(HER_FACTORS_AB_2040, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_2040 = Oldham_July_volume_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                 \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_2040 = Oldham_July_volume_2040.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_2040 = Oldham_July_24hr_matrix_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_2040[\"AB_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_2040 = Oldham_July_24hr_matrix_2040.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_2040 = pd.concat([Oldham_July_volume_Only_2040, Oldham_July_24hr_matrix_2040], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Oldham_July_Hourly_Volume_2040\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_2040)\n",
    "\n",
    "\n",
    "# Selecting needed files in Oldham BA\n",
    "Oldham_TRANSCAD_FILE_BA_2040 = Oldham_TRANSCAD_FILE_2040.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                   'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                   'BA_CAPDAY', \n",
    "                                                                   'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2040 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_BA_2040 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_BA_2040 = Oldham_HPMS_FACTORS_BA_2040.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2040 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_BA_2040['BA_FUNCL'] = Oldham_TRANSCAD_FILE_BA_2040['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2040['BA_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_BA_2040['BA_FUNCL'] = Oldham_HPMS_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2040['BA_FUNCL'] = HER_FACTORS_BA_2040['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_BA_2040 = Oldham_TRANSCAD_FILE_BA_2040.merge(Oldham_HPMS_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "Oldham_July_volume_BA_2040['BA_July_Volume'] = Oldham_July_volume_BA_2040.BA_VOL_Fin * Oldham_July_volume_BA_2040.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_BA_2040 = Oldham_July_volume_BA_2040.merge(ATR_HOURLY_VOLUME_BA_2040, on='BA_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_BA_2040 = Oldham_July_volume_BA_2040.merge(HER_FACTORS_BA_2040, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_BA_2040 = Oldham_July_volume_BA_2040.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                       \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                       \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_BA_2040 = Oldham_July_volume_BA_2040.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                       'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                       'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                       'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                       'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                       'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                       'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_BA_2040 = Oldham_July_24hr_matrix_BA_2040[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                      'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                      'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                      'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                      'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                      'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_BA_2040[\"BA_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_BA_2040 = Oldham_July_24hr_matrix_BA_2040.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_BA_2040 = pd.concat([Oldham_July_volume_Only_BA_2040, Oldham_July_24hr_matrix_BA_2040], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_BA_2040)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2040(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2040 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2040 = pd.concat([summary_df_2040, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2040\n",
    "\n",
    "# Call the function for AB direction in Oldham\n",
    "funcl_summary_Oldham_ab_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Oldham_July_Hourly_Volume_2040, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Oldham\n",
    "funcl_summary_Oldham_ba_2040 = sum_new_vmt_vht_and_speed_by_direction_2040(Oldham_July_Hourly_Volume_BA_2040, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Oldham_ab_2040 = funcl_summary_Oldham_ab_2040.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ab_2040 = funcl_summary_Oldham_ab_2040.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Oldham_ba_2040 = funcl_summary_Oldham_ba_2040.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ba_2040 = funcl_summary_Oldham_ba_2040.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2040 = pd.merge(funcl_summary_Oldham_ab_2040, funcl_summary_Oldham_ba_2040, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2040['Total_NEW_VMT'] = combined_summary_2040['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2040['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2040['Total_VHT'] = combined_summary_2040['Total_VHT_AB'].fillna(0) + combined_summary_2040['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2040 = combined_summary_2040['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2040 = combined_summary_2040['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2040['Mean_Congested_Speed'] = (mean_speed_ab_2040 + mean_speed_ba_2040) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Oldham\n",
    "Oldham_VMT_2040 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2040['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2040['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2040['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2040['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Oldham\n",
    "#print(Oldham_VMT)\n",
    "#Oldham_VMT\n",
    "\n",
    "# Assuming Oldham_VMT is defined elsewhere in your code\n",
    "Oldham_VMT_df_2040 = pd.DataFrame(Oldham_VMT_2040)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Oldham_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Oldham_VMT_df_2040 = pd.merge(expected_Oldham_VMT_df, Oldham_VMT_df_2040, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Oldham_VMT_df_2040[\"Total_NEW_VMT\"] = final_Oldham_VMT_df_2040[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Oldham_VMT_df_2040[\"Total_VHT\"] = final_Oldham_VMT_df_2040[\"Total_VHT\"].astype(float)\n",
    "final_Oldham_VMT_df_2040[\"Mean_Congested_Speed\"] = final_Oldham_VMT_df_2040[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Oldham_VMT_df_2040[final_Oldham_VMT_df_2040['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Oldham_VMT_df_2040[final_Oldham_VMT_df_2040['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Oldham_VMT_df_2040 = pd.concat([final_Oldham_VMT_df_2040, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Oldham_VMT_df_2040['Speed'] = final_Oldham_VMT_df_2040['Total_NEW_VMT'] / final_Oldham_VMT_df_2040['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Oldham_VMT_df_2040) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2040, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2040, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2040, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2040 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2040, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Oldham_VMT_df_2040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8851a041-0553-490b-a17b-79c0876d82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Oldham_VMT_df_2040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c6f09-dc9b-4c35-8379-d1410559e05e",
   "metadata": {},
   "source": [
    "# Combine 2040 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb3e0f91-83de-44fe-ae38-58703865d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Create a blank DataFrame with the same columns to use for spacing\n",
    "blank_lines = pd.DataFrame(np.nan, index=range(5), columns=Clark_VMT.columns)\n",
    "\n",
    "# Initialize a list to hold the DataFrames and their titles\n",
    "dataframes_with_titles = []\n",
    "\n",
    "# Append each DataFrame with its title and blank lines\n",
    "for county, df in zip(['Bullit', 'Clark', 'Floyd', 'Jefferson', 'Oldham'], \n",
    "                      [final_Bullit_VMT_df_2040, final_Clark_VMT_df_2040, final_Floyd_VMT_df_2040, final_Jefferson_VMT_df_2040, final_Oldham_VMT_df_2040]):\n",
    "    # Create a DataFrame for the title (bold)\n",
    "    title_df = pd.DataFrame(columns=df.columns)\n",
    "    title_df.loc[0] = df.columns  # Set the title row\n",
    "    # Append the county name with \"County\" as the title\n",
    "    dataframes_with_titles.append((f\"{county} County\", title_df, df, blank_lines))\n",
    "\n",
    "# Create a new Excel workbook\n",
    "output_file_path = '2040_Combined_VMT_Data.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = 'VMT_Data'\n",
    "\n",
    "# Add the dataframes and format them\n",
    "row_offset = 0\n",
    "for title, title_df, df, blanks in dataframes_with_titles:\n",
    "    # Add the file name with \"County\" as a subheading above each DataFrame\n",
    "    ws.cell(row=row_offset + 1, column=1, value=title).font = Font(bold=True)\n",
    "    row_offset += 1  # Move to the next row for the title\n",
    "\n",
    "    # Write the column titles (bold)\n",
    "    for c_idx, column_name in enumerate(title_df.columns, start=1):\n",
    "        ws.cell(row=row_offset + 1, column=c_idx, value=column_name).font = Font(bold=True)\n",
    "\n",
    "    # Write the DataFrame data\n",
    "    for r_idx, row in enumerate(df.values, start=row_offset + 2):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Adjust column widths based on the maximum length of data in each column\n",
    "    for c_idx in range(1, df.shape[1] + 1):\n",
    "        max_length = 0\n",
    "        # Check title length\n",
    "        max_length = max(max_length, len(title_df.columns[c_idx - 1]))  \n",
    "        # Check data length\n",
    "        for r_idx in range(df.shape[0]):\n",
    "            cell_value = str(df.iat[r_idx, c_idx - 1])  # Get cell value\n",
    "            max_length = max(max_length, len(cell_value))  # Update max length if necessary\n",
    "        ws.column_dimensions[ws.cell(row=1, column=c_idx).column_letter].width = max_length + 2  # Add some padding\n",
    "\n",
    "    # Update the row_offset for the next DataFrame\n",
    "    row_offset += df.shape[0] + 3  # 2 for the data and 1 for the title\n",
    "\n",
    "    # Add blank lines\n",
    "    for _ in range(5):\n",
    "        row_offset += 1  # Add blank lines between DataFrames\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef3cae-68b4-4d87-a142-9795788c8539",
   "metadata": {},
   "source": [
    "# 2050 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "655194d1-a594-4b53-81bb-03f69f70e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant variables\n",
    "TRANSCAD_RESULT_FILE_2050 = TRANSCAD_RESULT_FILE_2050.loc[:, ['ID', 'Length', 'Link_AADT', \n",
    "                                             'County', 'AB_FACT', 'FUNCL', 'BA_FACT', \n",
    "                                              'AB_FFSPD', 'BA_FFSPD', 'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                              'BA_VOL_Fin', 'Tot_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "\n",
    "# Fill null values in Link_AADT and convert types\n",
    "TRANSCAD_RESULT_FILE_2050['Link_AADT'] = TRANSCAD_RESULT_FILE_2050['Link_AADT'].fillna(0)\n",
    "TRANSCAD_RESULT_FILE_2050[\"Link_AADT\"] = TRANSCAD_RESULT_FILE_2050[\"Link_AADT\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"AB_CAPDAY\"] = TRANSCAD_RESULT_FILE_2050[\"AB_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"BA_CAPDAY\"] = TRANSCAD_RESULT_FILE_2050[\"BA_CAPDAY\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"FUNCL\"] = TRANSCAD_RESULT_FILE_2050[\"FUNCL\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"AB_FFSPD\"] = TRANSCAD_RESULT_FILE_2050[\"AB_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"BA_FFSPD\"] = TRANSCAD_RESULT_FILE_2050[\"BA_FFSPD\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"B_exponent\"] = TRANSCAD_RESULT_FILE_2050[\"B_exponent\"].astype(int)\n",
    "TRANSCAD_RESULT_FILE_2050[\"A_coefficient\"] = TRANSCAD_RESULT_FILE_2050[\"A_coefficient\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2050[\"BA_VOL_Fin\"] = TRANSCAD_RESULT_FILE_2050[\"BA_VOL_Fin\"].astype(float)\n",
    "TRANSCAD_RESULT_FILE_2050[\"Length\"] = TRANSCAD_RESULT_FILE_2050[\"Length\"].astype(float)\n",
    "\n",
    "\n",
    "# Calculate Peak Hour Capacity\n",
    "TRANSCAD_RESULT_FILE_2050['AB_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2050.AB_CAPDAY / 10\n",
    "TRANSCAD_RESULT_FILE_2050['BA_Peak_HR_Capacity'] = TRANSCAD_RESULT_FILE_2050.BA_CAPDAY / 10\n",
    "\n",
    "# Create duplicate points column\n",
    "TRANSCAD_RESULT_FILE_2050['AB_FUNCL'] = TRANSCAD_RESULT_FILE_2050.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2050['BA_FUNCL'] = TRANSCAD_RESULT_FILE_2050.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2050['AB_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2050.loc[:, 'FUNCL']\n",
    "TRANSCAD_RESULT_FILE_2050['BA_FUNCL_ATR'] = TRANSCAD_RESULT_FILE_2050.loc[:, 'FUNCL']\n",
    "\n",
    "#Library original\n",
    "TRANSCAD_RESULT_FILE_2050[\"AB_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                   8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                   18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "#Library\n",
    "TRANSCAD_RESULT_FILE_2050[\"BA_FUNCL\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL FREEWAYS/X-WAYS', 3:'RURAL PRINCIPAL ARTERIALS', 4:'RURAL MINOR ARTERIALS', 5:'RURAL MAJOR COLLECTORS', 6:'RURAL MINOR COLLECTORS', 7:'RURAL LOCAL ROADS', \n",
    "                                  8:'RURAL CENTROID CONNECTORS', 9:'RURAL RAMPS',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MINOR COLLECTORS',17:'URBAN LOCAL ROADS',\n",
    "                                  18:'URBAN CENTROID CONNECTORS',19:'URBAN RAMPS',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2050[\"AB_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n",
    "\n",
    "TRANSCAD_RESULT_FILE_2050[\"BA_FUNCL_ATR\"].replace({1: 'RURAL INTERSTATES', 2: 'RURAL INTERSTATES', 3:'RURAL FREEWAYS/X-WAYS', 4:'RURAL PRINCIPAL ARTERIALS', 5:'RURAL MINOR ARTERIALS', 6:'RURAL MAJOR COLLECTORS', 7:'RURAL MINOR COLLECTORS', \n",
    "                                   8:'RURAL MINOR COLLECTORS', 9:'RURAL INTERSTATES',10:'NOT USED',11:'URBAN INTERSTATES',12:'URBAN FREEWAYS/X-WAYS',13:'URBAN PRINCIPAL ARTERIALS',14:'URBAN MINOR ARTERIALS',15:'URBAN MAJOR COLLECTORS',16:'URBAN MAJOR COLLECTORS',17:'URBAN MINOR COLLECTORS',\n",
    "                                   18:'URBAN MINOR COLLECTORS',19:'URBAN INTERSTATES',20:'NOT USED'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e770d9a-4524-4081-8c00-d9eddeb33c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating different county files \n",
    "Bullit_TRANSCAD_FILE_2050 = TRANSCAD_RESULT_FILE_2050.loc[TRANSCAD_RESULT_FILE_2050.County == 'Bullitt', :]\n",
    "\n",
    "Clark_TRANSCAD_FILE_2050 = TRANSCAD_RESULT_FILE_2050.loc[TRANSCAD_RESULT_FILE_2050.County == 'Clark', :]\n",
    "\n",
    "Floyd_TRANSCAD_FILE_2050 = TRANSCAD_RESULT_FILE_2050.loc[TRANSCAD_RESULT_FILE_2050.County == 'Floyd', :]\n",
    "\n",
    "Jefferson_TRANSCAD_FILE_2050 = TRANSCAD_RESULT_FILE_2050.loc[TRANSCAD_RESULT_FILE_2050.County == 'Jefferson', :]\n",
    "\n",
    "Oldham_TRANSCAD_FILE_2050 = TRANSCAD_RESULT_FILE_2050.loc[TRANSCAD_RESULT_FILE_2050.County == 'Oldham', :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2355971-81cc-48d2-9f9f-2fe89ce49c1e",
   "metadata": {},
   "source": [
    "# 2050 Bullit Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb702afb-f8db-447b-8e91-50a53c04b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\2365735334.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Bullit \n",
    "# Selecting needed files in Bullit AB\n",
    "Bullit_TRANSCAD_FILE_AB_2050 = Bullit_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                  'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                  'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_AB_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_AB_2050 = Bullit_HPMS_FACTORS_AB_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_AB_2050['AB_FUNCL'] = Bullit_TRANSCAD_FILE_AB_2050['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_AB_2050['AB_FUNCL'] = Bullit_HPMS_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2050['AB_FUNCL'] = HER_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume_2050 = Bullit_TRANSCAD_FILE_AB_2050.merge(Bullit_HPMS_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "Bullit_July_volume_2050['AB_July_Volume'] = Bullit_July_volume_2050.AB_VOL_Fin * Bullit_July_volume_2050.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_2050 = Bullit_July_volume_2050.merge(ATR_HOURLY_VOLUME_AB_2050, on='AB_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_2050 = Bullit_July_volume_2050.merge(HER_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_2050 = Bullit_July_volume_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                 \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_2050 = Bullit_July_volume_2050.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_2050 = Bullit_July_24hr_matrix_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_2050[\"AB_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_2050 = Bullit_July_24hr_matrix_2050.fillna(0)\n",
    "\n",
    "Bullit_July_Hourly_Volume_2050 = pd.concat([Bullit_July_volume_Only_2050, Bullit_July_24hr_matrix_2050], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Bullit_July_Hourly_Volume_2050\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_2050)\n",
    "\n",
    "\n",
    "# Selecting needed files in Bullit BA\n",
    "Bullit_TRANSCAD_FILE_BA_2050 = Bullit_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                  'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                  'BA_CAPDAY', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Bullit HPMS to adjust the AADT to July Daily Volume\n",
    "Bullit_HPMS_FACTORS_BA_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'BULLIT']]\n",
    "Bullit_HPMS_FACTORS_BA_2050 = Bullit_HPMS_FACTORS_BA_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Bullit_TRANSCAD_FILE_BA_2050['BA_FUNCL'] = Bullit_TRANSCAD_FILE_BA_2050['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'].astype(str)\n",
    "Bullit_HPMS_FACTORS_BA_2050['BA_FUNCL'] = Bullit_HPMS_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2050['BA_FUNCL'] = HER_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Bullit_July_volume_BA_2050 = Bullit_TRANSCAD_FILE_BA_2050.merge(Bullit_HPMS_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "Bullit_July_volume_BA_2050['BA_July_Volume'] = Bullit_July_volume_BA_2050.BA_VOL_Fin * Bullit_July_volume_BA_2050.BULLIT\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Bullit_July_volume_BA_2050 = Bullit_July_volume_BA_2050.merge(ATR_HOURLY_VOLUME_BA_2050, on='BA_FUNCL_ATR', how='left')\n",
    "Bullit_July_volume_BA_2050 = Bullit_July_volume_BA_2050.merge(HER_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Bullit_July_volume_Only_BA_2050 = Bullit_July_volume_BA_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                       \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                       \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Bullit July 24-hour matrix\n",
    "Bullit_July_24hr_matrix_BA_2050 = Bullit_July_volume_BA_2050.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                       'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                       'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                       'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                       'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                       'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                       'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Bullit_July_24hr_matrix_BA_2050 = Bullit_July_24hr_matrix_BA_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                      'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                      'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                      'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                      'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                      'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Bullit_July_24hr_matrix_BA_2050[\"BA_July_Volume\"], axis=\"index\")\n",
    "Bullit_July_24hr_matrix_BA_2050 = Bullit_July_24hr_matrix_BA_2050.fillna(0)\n",
    "\n",
    "Bullit_July_Hourly_Volume_BA_2050 = pd.concat([Bullit_July_volume_Only_BA_2050, Bullit_July_24hr_matrix_BA_2050], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Bullit_July_Hourly_Volume_BA_2050)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2050(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2050 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2050\n",
    "\n",
    "# Call the function for AB direction in Bullit\n",
    "funcl_summary_Bullit_ab_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Bullit_July_Hourly_Volume_2050, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Bullit\n",
    "funcl_summary_Bullit_ba_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Bullit_July_Hourly_Volume_BA_2050, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Bullit_ab_2050 = funcl_summary_Bullit_ab_2050.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ab_2050 = funcl_summary_Bullit_ab_2050.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Bullit_ba_2050 = funcl_summary_Bullit_ba_2050.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Bullit_ba_2050 = funcl_summary_Bullit_ba_2050.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2050 = pd.merge(funcl_summary_Bullit_ab_2050, funcl_summary_Bullit_ba_2050, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2050['Total_NEW_VMT'] = combined_summary_2050['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2050['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2050['Total_VHT'] = combined_summary_2050['Total_VHT_AB'].fillna(0) + combined_summary_2050['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2050 = combined_summary_2050['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2050 = combined_summary_2050['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2050['Mean_Congested_Speed'] = (mean_speed_ab_2050 + mean_speed_ba_2050) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Bullit\n",
    "Bullit_VMT_2050 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2050['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2050['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2050['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2050['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Bullit\n",
    "#print(Bullit_VMT)\n",
    "#Bullit_VMT\n",
    "\n",
    "# Assuming Bullit_VMT is defined elsewhere in your code\n",
    "Bullit_VMT_df_2050 = pd.DataFrame(Bullit_VMT_2050)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Bullit_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Bullit_VMT_df_2050 = pd.merge(expected_Bullit_VMT_df, Bullit_VMT_df_2050, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Bullit_VMT_df_2050[\"Total_NEW_VMT\"] = final_Bullit_VMT_df_2050[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Bullit_VMT_df_2050[\"Total_VHT\"] = final_Bullit_VMT_df_2050[\"Total_VHT\"].astype(float)\n",
    "final_Bullit_VMT_df_2050[\"Mean_Congested_Speed\"] = final_Bullit_VMT_df_2050[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Bullit_VMT_df_2050[final_Bullit_VMT_df_2050['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Bullit_VMT_df_2050[final_Bullit_VMT_df_2050['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Bullit_VMT_df_2050 = pd.concat([final_Bullit_VMT_df_2050, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Bullit_VMT_df_2050['Speed'] = final_Bullit_VMT_df_2050['Total_NEW_VMT'] / final_Bullit_VMT_df_2050['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Bullit_VMT_df_2050) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2050, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Bullit_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2050, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2050, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Bullit_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Bullit_VMT_df_2050, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Bullit_VMT_df_2050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e86f242-9f7e-48e8-b26d-06ddcb936403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Bullit_VMT_df_2050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d4849-2a0a-4aae-9eee-935c58ef838f",
   "metadata": {},
   "source": [
    "# 2050 Clark Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75c18af4-2b72-4853-9e9b-c315fc53b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\752212552.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Clark \n",
    "# Selecting needed files in Clark AB\n",
    "Clark_TRANSCAD_FILE_AB_2050 = Clark_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_AB_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_AB_2050 = Clark_HPMS_FACTORS_AB_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_AB_2050['AB_FUNCL'] = Clark_TRANSCAD_FILE_AB_2050['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_AB_2050['AB_FUNCL'] = Clark_HPMS_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2050['AB_FUNCL'] = HER_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_2050 = Clark_TRANSCAD_FILE_AB_2050.merge(Clark_HPMS_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "Clark_July_volume_2050['AB_July_Volume'] = Clark_July_volume_2050.AB_VOL_Fin * Clark_July_volume_2050.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_2050 = Clark_July_volume_2050.merge(ATR_HOURLY_VOLUME_AB_2050, on='AB_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_2050 = Clark_July_volume_2050.merge(HER_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_2050 = Clark_July_volume_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                               \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                               \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_2050 = Clark_July_volume_2050.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                               'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                               'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                               'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                               'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                               'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                               'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_2050 = Clark_July_24hr_matrix_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_2050[\"AB_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_2050 = Clark_July_24hr_matrix_2050.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_2050 = pd.concat([Clark_July_volume_Only_2050, Clark_July_24hr_matrix_2050], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Clark_July_Hourly_Volume_2050\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_2050)\n",
    "\n",
    "\n",
    "# Selecting needed files in Clark BA\n",
    "Clark_TRANSCAD_FILE_BA_2050 = Clark_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                  'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                  'BA_CAPDAY', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Clark HPMS to adjust the AADT to July Daily Volume\n",
    "Clark_HPMS_FACTORS_BA_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'CLARK']]\n",
    "Clark_HPMS_FACTORS_BA_2050 = Clark_HPMS_FACTORS_BA_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Clark_TRANSCAD_FILE_BA_2050['BA_FUNCL'] = Clark_TRANSCAD_FILE_BA_2050['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'].astype(str)\n",
    "Clark_HPMS_FACTORS_BA_2050['BA_FUNCL'] = Clark_HPMS_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2050['BA_FUNCL'] = HER_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Clark_July_volume_BA_2050 = Clark_TRANSCAD_FILE_BA_2050.merge(Clark_HPMS_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "Clark_July_volume_BA_2050['BA_July_Volume'] = Clark_July_volume_BA_2050.BA_VOL_Fin * Clark_July_volume_BA_2050.CLARK\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Clark_July_volume_BA_2050 = Clark_July_volume_BA_2050.merge(ATR_HOURLY_VOLUME_BA_2050, on='BA_FUNCL_ATR', how='left')\n",
    "Clark_July_volume_BA_2050 = Clark_July_volume_BA_2050.merge(HER_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Clark_July_volume_Only_BA_2050 = Clark_July_volume_BA_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                     \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                     \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Clark July 24-hour matrix\n",
    "Clark_July_24hr_matrix_BA_2050 = Clark_July_volume_BA_2050.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                     'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                     'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                     'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                     'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                     'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                     'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Clark_July_24hr_matrix_BA_2050 = Clark_July_24hr_matrix_BA_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                    'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                    'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                    'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                    'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                    'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Clark_July_24hr_matrix_BA_2050[\"BA_July_Volume\"], axis=\"index\")\n",
    "Clark_July_24hr_matrix_BA_2050 = Clark_July_24hr_matrix_BA_2050.fillna(0)\n",
    "\n",
    "Clark_July_Hourly_Volume_BA_2050 = pd.concat([Clark_July_volume_Only_BA_2050, Clark_July_24hr_matrix_BA_2050], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Clark_July_Hourly_Volume_BA_2050)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2050(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2050 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2050\n",
    "\n",
    "# Call the function for AB direction in Clark\n",
    "funcl_summary_Clark_ab_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Clark_July_Hourly_Volume_2050, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Clark\n",
    "funcl_summary_Clark_ba_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Clark_July_Hourly_Volume_BA_2050, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Clark_ab_2050 = funcl_summary_Clark_ab_2050.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ab_2050 = funcl_summary_Clark_ab_2050.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Clark_ba_2050 = funcl_summary_Clark_ba_2050.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Clark_ba_2050 = funcl_summary_Clark_ba_2050.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2050 = pd.merge(funcl_summary_Clark_ab_2050, funcl_summary_Clark_ba_2050, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2050['Total_NEW_VMT'] = combined_summary_2050['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2050['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2050['Total_VHT'] = combined_summary_2050['Total_VHT_AB'].fillna(0) + combined_summary_2050['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2050 = combined_summary_2050['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2050 = combined_summary_2050['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2050['Mean_Congested_Speed'] = (mean_speed_ab_2050 + mean_speed_ba_2050) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Clark\n",
    "Clark_VMT_2050 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2050['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2050['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2050['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2050['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Clark\n",
    "#print(Clark_VMT)\n",
    "#Clark_VMT\n",
    "\n",
    "# Assuming Clark_VMT is defined elsewhere in your code\n",
    "Clark_VMT_df_2050 = pd.DataFrame(Clark_VMT_2050)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Clark_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Clark_VMT_df_2050 = pd.merge(expected_Clark_VMT_df, Clark_VMT_df_2050, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Clark_VMT_df_2050[\"Total_NEW_VMT\"] = final_Clark_VMT_df_2050[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Clark_VMT_df_2050[\"Total_VHT\"] = final_Clark_VMT_df_2050[\"Total_VHT\"].astype(float)\n",
    "final_Clark_VMT_df_2050[\"Mean_Congested_Speed\"] = final_Clark_VMT_df_2050[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Clark_VMT_df_2050[final_Clark_VMT_df_2050['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Clark_VMT_df_2050[final_Clark_VMT_df_2050['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Clark_VMT_df_2050 = pd.concat([final_Clark_VMT_df_2050, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Clark_VMT_df_2050['Speed'] = final_Clark_VMT_df_2050['Total_NEW_VMT'] / final_Clark_VMT_df_2050['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Clark_VMT_df_2050) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2050, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Clark_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2050, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2050, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Clark_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Clark_VMT_df_2050, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Clark_VMT_df_2050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ed76458-34cd-4e52-b909-8fa8616ddd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Clark_VMT_df_2050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ff87f-96c4-44cf-a3ef-c34840aa0b20",
   "metadata": {},
   "source": [
    "# 2050 Floyd Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "af4ba738-83a7-4d45-bc22-c36f917e7325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:339: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] +\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:351: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\1003977057.py:352: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n"
     ]
    }
   ],
   "source": [
    "# Floyd \n",
    "# Selecting needed files in Floyd AB\n",
    "Floyd_TRANSCAD_FILE_AB_2050 = Floyd_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                               'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                               'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                               'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_AB_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_AB_2050 = Floyd_HPMS_FACTORS_AB_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_AB_2050['AB_FUNCL'] = Floyd_TRANSCAD_FILE_AB_2050['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_AB_2050['AB_FUNCL'] = Floyd_HPMS_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2050['AB_FUNCL'] = HER_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_2050 = Floyd_TRANSCAD_FILE_AB_2050.merge(Floyd_HPMS_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "Floyd_July_volume_2050['AB_July_Volume'] = Floyd_July_volume_2050.AB_VOL_Fin * Floyd_July_volume_2050.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_2050 = Floyd_July_volume_2050.merge(ATR_HOURLY_VOLUME_AB_2050, on='AB_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_2050 = Floyd_July_volume_2050.merge(HER_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_2050 = Floyd_July_volume_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                               \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                               \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_2050 = Floyd_July_volume_2050.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                               'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                               'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                               'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                               'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                               'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                               'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_2050 = Floyd_July_24hr_matrix_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_2050[\"AB_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_2050 = Floyd_July_24hr_matrix_2050.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_2050 = pd.concat([Floyd_July_volume_Only_2050, Floyd_July_24hr_matrix_2050], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Floyd_July_Hourly_Volume_2050\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_2050)\n",
    "\n",
    "\n",
    "# Selecting needed files in Floyd BA\n",
    "Floyd_TRANSCAD_FILE_BA_2050 = Floyd_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                  'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                  'BA_CAPDAY', \n",
    "                                                                  'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Floyd HPMS to adjust the AADT to July Daily Volume\n",
    "Floyd_HPMS_FACTORS_BA_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'FLOYD']]\n",
    "Floyd_HPMS_FACTORS_BA_2050 = Floyd_HPMS_FACTORS_BA_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Floyd_TRANSCAD_FILE_BA_2050['BA_FUNCL'] = Floyd_TRANSCAD_FILE_BA_2050['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'].astype(str)\n",
    "Floyd_HPMS_FACTORS_BA_2050['BA_FUNCL'] = Floyd_HPMS_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2050['BA_FUNCL'] = HER_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Floyd_July_volume_BA_2050 = Floyd_TRANSCAD_FILE_BA_2050.merge(Floyd_HPMS_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "Floyd_July_volume_BA_2050['BA_July_Volume'] = Floyd_July_volume_BA_2050.BA_VOL_Fin * Floyd_July_volume_BA_2050.FLOYD\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Floyd_July_volume_BA_2050 = Floyd_July_volume_BA_2050.merge(ATR_HOURLY_VOLUME_BA_2050, on='BA_FUNCL_ATR', how='left')\n",
    "Floyd_July_volume_BA_2050 = Floyd_July_volume_BA_2050.merge(HER_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Floyd_July_volume_Only_BA_2050 = Floyd_July_volume_BA_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                     \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                     \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Floyd July 24-hour matrix\n",
    "Floyd_July_24hr_matrix_BA_2050 = Floyd_July_volume_BA_2050.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                     'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                     'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                     'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                     'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                     'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                     'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Floyd_July_24hr_matrix_BA_2050 = Floyd_July_24hr_matrix_BA_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                    'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                    'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                    'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                    'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                    'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Floyd_July_24hr_matrix_BA_2050[\"BA_July_Volume\"], axis=\"index\")\n",
    "Floyd_July_24hr_matrix_BA_2050 = Floyd_July_24hr_matrix_BA_2050.fillna(0)\n",
    "\n",
    "Floyd_July_Hourly_Volume_BA_2050 = pd.concat([Floyd_July_volume_Only_BA_2050, Floyd_July_24hr_matrix_BA_2050], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Floyd_July_Hourly_Volume_BA_2050)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2050(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2050 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2050\n",
    "\n",
    "# Call the function for AB direction in Floyd\n",
    "funcl_summary_Floyd_ab_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Floyd_July_Hourly_Volume_2050, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Floyd\n",
    "funcl_summary_Floyd_ba_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Floyd_July_Hourly_Volume_BA_2050, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Floyd_ab_2050 = funcl_summary_Floyd_ab_2050.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ab_2050 = funcl_summary_Floyd_ab_2050.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Floyd_ba_2050 = funcl_summary_Floyd_ba_2050.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Floyd_ba_2050 = funcl_summary_Floyd_ba_2050.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2050 = pd.merge(funcl_summary_Floyd_ab_2050, funcl_summary_Floyd_ba_2050, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2050['Total_NEW_VMT'] = combined_summary_2050['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2050['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2050['Total_VHT'] = combined_summary_2050['Total_VHT_AB'].fillna(0) + combined_summary_2050['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2050 = combined_summary_2050['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2050 = combined_summary_2050['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2050['Mean_Congested_Speed'] = (mean_speed_ab_2050 + mean_speed_ba_2050) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Floyd\n",
    "Floyd_VMT_2050 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2050['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2050['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2050['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2050['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Floyd\n",
    "#print(Floyd_VMT)\n",
    "#Floyd_VMT\n",
    "\n",
    "# Assuming Floyd_VMT is defined elsewhere in your code\n",
    "Floyd_VMT_df_2050 = pd.DataFrame(Floyd_VMT_2050)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Floyd_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Floyd_VMT_df_2050 = pd.merge(expected_Floyd_VMT_df, Floyd_VMT_df_2050, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Floyd_VMT_df_2050[\"Total_NEW_VMT\"] = final_Floyd_VMT_df_2050[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Floyd_VMT_df_2050[\"Total_VHT\"] = final_Floyd_VMT_df_2050[\"Total_VHT\"].astype(float)\n",
    "final_Floyd_VMT_df_2050[\"Mean_Congested_Speed\"] = final_Floyd_VMT_df_2050[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Floyd_VMT_df_2050[final_Floyd_VMT_df_2050['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Floyd_VMT_df_2050[final_Floyd_VMT_df_2050['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Floyd_VMT_df_2050 = pd.concat([final_Floyd_VMT_df_2050, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Floyd_VMT_df_2050['Speed'] = final_Floyd_VMT_df_2050['Total_NEW_VMT'] / final_Floyd_VMT_df_2050['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Floyd_VMT_df_2050) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2050, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Floyd_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2050, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2050, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Floyd_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Floyd_VMT_df_2050, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Floyd_VMT_df_2050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3477cf24-6cb1-43e1-afa6-1b2d5057efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Floyd_VMT_df_2050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bad98-bc86-478f-8ef0-6e21aeae1154",
   "metadata": {},
   "source": [
    "# 2050 Jefferson Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3399357c-ee0b-44f1-881a-c12a3d8f57e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\266208862.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Jefferson \n",
    "# Selecting needed files in Jefferson AB\n",
    "Jefferson_TRANSCAD_FILE_AB_2050 = Jefferson_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                       'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                       'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                       'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_AB_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_AB_2050 = Jefferson_HPMS_FACTORS_AB_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_AB_2050['AB_FUNCL'] = Jefferson_TRANSCAD_FILE_AB_2050['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_AB_2050['AB_FUNCL'] = Jefferson_HPMS_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2050['AB_FUNCL'] = HER_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_2050 = Jefferson_TRANSCAD_FILE_AB_2050.merge(Jefferson_HPMS_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "Jefferson_July_volume_2050['AB_July_Volume'] = Jefferson_July_volume_2050.AB_VOL_Fin * Jefferson_July_volume_2050.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_2050 = Jefferson_July_volume_2050.merge(ATR_HOURLY_VOLUME_AB_2050, on='AB_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_2050 = Jefferson_July_volume_2050.merge(HER_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_2050 = Jefferson_July_volume_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                     \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                     \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_2050 = Jefferson_July_volume_2050.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                     'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                     'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                     'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                     'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                     'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                     'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_2050 = Jefferson_July_24hr_matrix_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                    'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                    'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                    'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                    'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                    'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_2050[\"AB_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_2050 = Jefferson_July_24hr_matrix_2050.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2050 = pd.concat([Jefferson_July_volume_Only_2050, Jefferson_July_24hr_matrix_2050], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Jefferson_July_Hourly_Volume_2050\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_2050)\n",
    "\n",
    "\n",
    "# Selecting needed files in Jefferson BA\n",
    "Jefferson_TRANSCAD_FILE_BA_2050 = Jefferson_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                        'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                        'BA_CAPDAY', \n",
    "                                                                        'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Jefferson HPMS to adjust the AADT to July Daily Volume\n",
    "Jefferson_HPMS_FACTORS_BA_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'JEFF']]\n",
    "Jefferson_HPMS_FACTORS_BA_2050 = Jefferson_HPMS_FACTORS_BA_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Jefferson_TRANSCAD_FILE_BA_2050['BA_FUNCL'] = Jefferson_TRANSCAD_FILE_BA_2050['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'].astype(str)\n",
    "Jefferson_HPMS_FACTORS_BA_2050['BA_FUNCL'] = Jefferson_HPMS_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2050['BA_FUNCL'] = HER_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Jefferson_July_volume_BA_2050 = Jefferson_TRANSCAD_FILE_BA_2050.merge(Jefferson_HPMS_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "Jefferson_July_volume_BA_2050['BA_July_Volume'] = Jefferson_July_volume_BA_2050.BA_VOL_Fin * Jefferson_July_volume_BA_2050.JEFF\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Jefferson_July_volume_BA_2050 = Jefferson_July_volume_BA_2050.merge(ATR_HOURLY_VOLUME_BA_2050, on='BA_FUNCL_ATR', how='left')\n",
    "Jefferson_July_volume_BA_2050 = Jefferson_July_volume_BA_2050.merge(HER_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Jefferson_July_volume_Only_BA_2050 = Jefferson_July_volume_BA_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                             \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                             \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Jefferson July 24-hour matrix\n",
    "Jefferson_July_24hr_matrix_BA_2050 = Jefferson_July_volume_BA_2050.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                             'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                             'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                             'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                             'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                             'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Jefferson_July_24hr_matrix_BA_2050 = Jefferson_July_24hr_matrix_BA_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                            'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                            'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                            'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                            'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                            'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Jefferson_July_24hr_matrix_BA_2050[\"BA_July_Volume\"], axis=\"index\")\n",
    "Jefferson_July_24hr_matrix_BA_2050 = Jefferson_July_24hr_matrix_BA_2050.fillna(0)\n",
    "\n",
    "Jefferson_July_Hourly_Volume_BA_2050 = pd.concat([Jefferson_July_volume_Only_BA_2050, Jefferson_July_24hr_matrix_BA_2050], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Jefferson_July_Hourly_Volume_BA_2050)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2050(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2050 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2050\n",
    "\n",
    "# Call the function for AB direction in Jefferson\n",
    "funcl_summary_Jefferson_ab_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Jefferson_July_Hourly_Volume_2050, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Jefferson\n",
    "funcl_summary_Jefferson_ba_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Jefferson_July_Hourly_Volume_BA_2050, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Jefferson_ab_2050 = funcl_summary_Jefferson_ab_2050.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ab_2050 = funcl_summary_Jefferson_ab_2050.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Jefferson_ba_2050 = funcl_summary_Jefferson_ba_2050.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Jefferson_ba_2050 = funcl_summary_Jefferson_ba_2050.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2050 = pd.merge(funcl_summary_Jefferson_ab_2050, funcl_summary_Jefferson_ba_2050, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2050['Total_NEW_VMT'] = combined_summary_2050['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2050['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2050['Total_VHT'] = combined_summary_2050['Total_VHT_AB'].fillna(0) + combined_summary_2050['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2050 = combined_summary_2050['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2050 = combined_summary_2050['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2050['Mean_Congested_Speed'] = (mean_speed_ab_2050 + mean_speed_ba_2050) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Jefferson\n",
    "Jefferson_VMT_2050 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2050['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2050['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2050['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2050['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Jefferson\n",
    "#print(Jefferson_VMT)\n",
    "#Jefferson_VMT\n",
    "\n",
    "# Assuming Jefferson_VMT is defined elsewhere in your code\n",
    "Jefferson_VMT_df_2050 = pd.DataFrame(Jefferson_VMT_2050)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Jefferson_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Jefferson_VMT_df_2050 = pd.merge(expected_Jefferson_VMT_df, Jefferson_VMT_df_2050, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Jefferson_VMT_df_2050[\"Total_NEW_VMT\"] = final_Jefferson_VMT_df_2050[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2050[\"Total_VHT\"] = final_Jefferson_VMT_df_2050[\"Total_VHT\"].astype(float)\n",
    "final_Jefferson_VMT_df_2050[\"Mean_Congested_Speed\"] = final_Jefferson_VMT_df_2050[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Jefferson_VMT_df_2050[final_Jefferson_VMT_df_2050['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Jefferson_VMT_df_2050[final_Jefferson_VMT_df_2050['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Jefferson_VMT_df_2050 = pd.concat([final_Jefferson_VMT_df_2050, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Jefferson_VMT_df_2050['Speed'] = final_Jefferson_VMT_df_2050['Total_NEW_VMT'] / final_Jefferson_VMT_df_2050['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Jefferson_VMT_df_2050) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2050, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Jefferson_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2050, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2050, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Jefferson_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Jefferson_VMT_df_2050, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Jefferson_VMT_df_2050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f731c1e-d901-42e5-af8a-febd42ce19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Jefferson_VMT_df_2050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f0163-796f-4a11-b0d5-7f1f0236c5e3",
   "metadata": {},
   "source": [
    "# 2050 Oldham Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "707dc8bd-33f7-4782-b4c9-8d36e624e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
      "C:\\Users\\eronmonsele.esekhaig\\AppData\\Local\\Temp\\ipykernel_35688\\858511486.py:193: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Oldham \n",
    "# Selecting needed files in Oldham AB\n",
    "Oldham_TRANSCAD_FILE_AB_2050 = Oldham_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'AB_Peak_HR_Capacity',\n",
    "                                                                   'AB_FUNCL', 'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'AB_FFSPD', 'BA_FFSPD', \n",
    "                                                                   'AB_CAPDAY', 'BA_CAPDAY', 'AB_VOL_Fin', \n",
    "                                                                   'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_AB_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'AB_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_AB_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_AB_2050 = Oldham_HPMS_FACTORS_AB_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "HER_FACTORS_AB_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'AB_FUNCL'})\n",
    "\n",
    "# Ensure 'AB_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_AB_2050['AB_FUNCL'] = Oldham_TRANSCAD_FILE_AB_2050['AB_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'] = ATR_HOURLY_VOLUME_AB_2050['AB_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_AB_2050['AB_FUNCL'] = Oldham_HPMS_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "HER_FACTORS_AB_2050['AB_FUNCL'] = HER_FACTORS_AB_2050['AB_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_2050 = Oldham_TRANSCAD_FILE_AB_2050.merge(Oldham_HPMS_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "Oldham_July_volume_2050['AB_July_Volume'] = Oldham_July_volume_2050.AB_VOL_Fin * Oldham_July_volume_2050.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_2050 = Oldham_July_volume_2050.merge(ATR_HOURLY_VOLUME_AB_2050, on='AB_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_2050 = Oldham_July_volume_2050.merge(HER_FACTORS_AB_2050, on='AB_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_2050 = Oldham_July_volume_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"AB_FUNCL\", 'AB_FUNCL_ATR',\n",
    "                                                                 \"AB_July_Volume\", \"AB_Peak_HR_Capacity\", \"AB_FFSPD\", \n",
    "                                                                 \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_2050 = Oldham_July_volume_2050.loc[:, [\"ID\", \"AB_FUNCL\", \"AB_July_Volume\", 'AB_FUNCL_ATR',\n",
    "                                                                 'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                 'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_2050 = Oldham_July_24hr_matrix_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                              'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                              'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                              'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                              'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                              'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_2050[\"AB_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_2050 = Oldham_July_24hr_matrix_2050.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_2050 = pd.concat([Oldham_July_volume_Only_2050, Oldham_July_24hr_matrix_2050], axis=1, join='inner')\n",
    "\n",
    "\n",
    "Oldham_July_Hourly_Volume_2050\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_AB_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_AB_New_VMT'\n",
    "    vht_col = f'{hour}_AB_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.AB_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.AB_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "    \n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM','Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_2050)\n",
    "\n",
    "\n",
    "# Selecting needed files in Oldham BA\n",
    "Oldham_TRANSCAD_FILE_BA_2050 = Oldham_TRANSCAD_FILE_2050.loc[:, ['ID', 'Length', 'BA_Peak_HR_Capacity', \n",
    "                                                                    'BA_FUNCL', 'AB_FUNCL_ATR', 'BA_FUNCL_ATR', 'BA_FFSPD', \n",
    "                                                                    'BA_CAPDAY', \n",
    "                                                                    'BA_VOL_Fin', 'A_coefficient', 'B_exponent']]\n",
    "\n",
    "# Transpose the ATR\n",
    "ATR_HOURLY_VOLUME_BA_2050 = ATR_HOURLY_VOLUME.rename(columns={'Time': 'BA_FUNCL_ATR'})\n",
    "\n",
    "# Adjusting the Oldham HPMS to adjust the AADT to July Daily Volume\n",
    "Oldham_HPMS_FACTORS_BA_2050 = HPMS_FACTORS.loc[:, ['FUNCTIONAL_CLASSIFICATION', 'OLDHAM']]\n",
    "Oldham_HPMS_FACTORS_BA_2050 = Oldham_HPMS_FACTORS_BA_2050.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "HER_FACTORS_BA_2050 = HER_FACTORS.rename(columns={'FUNCTIONAL_CLASSIFICATION': 'BA_FUNCL'})\n",
    "\n",
    "# Ensure 'BA_FUNCL' is of type string in both DataFrames before merging\n",
    "Oldham_TRANSCAD_FILE_BA_2050['BA_FUNCL'] = Oldham_TRANSCAD_FILE_BA_2050['BA_FUNCL'].astype(str)\n",
    "ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'] = ATR_HOURLY_VOLUME_BA_2050['BA_FUNCL_ATR'].astype(str)\n",
    "Oldham_HPMS_FACTORS_BA_2050['BA_FUNCL'] = Oldham_HPMS_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "HER_FACTORS_BA_2050['BA_FUNCL'] = HER_FACTORS_BA_2050['BA_FUNCL'].astype(str)\n",
    "\n",
    "# Create the adjusted volume for the month of July using the HPMS file. \n",
    "Oldham_July_volume_BA_2050 = Oldham_TRANSCAD_FILE_BA_2050.merge(Oldham_HPMS_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "Oldham_July_volume_BA_2050['BA_July_Volume'] = Oldham_July_volume_BA_2050.BA_VOL_Fin * Oldham_July_volume_BA_2050.OLDHAM\n",
    "\n",
    "# Merge with ATR and HER factors\n",
    "Oldham_July_volume_BA_2050 = Oldham_July_volume_BA_2050.merge(ATR_HOURLY_VOLUME_BA_2050, on='BA_FUNCL_ATR', how='left')\n",
    "Oldham_July_volume_BA_2050 = Oldham_July_volume_BA_2050.merge(HER_FACTORS_BA_2050, on='BA_FUNCL', how='left')\n",
    "\n",
    "# Select relevant columns for output\n",
    "Oldham_July_volume_Only_BA_2050 = Oldham_July_volume_BA_2050.loc[:, [\"ID\", \"Length\", \"Speed_Adjustement_Factor\", \"BA_FUNCL\", 'BA_FUNCL_ATR',\n",
    "                                                                        \"BA_July_Volume\", \"BA_Peak_HR_Capacity\", \"BA_FFSPD\", \n",
    "                                                                        \"A_coefficient\", \"B_exponent\"]]\n",
    "\n",
    "# Select Oldham July 24-hour matrix\n",
    "Oldham_July_24hr_matrix_BA_2050 = Oldham_July_volume_BA_2050.loc[:, [\"ID\", \"BA_FUNCL\", \"BA_July_Volume\", 'BA_FUNCL_ATR',\n",
    "                                                                        'Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                        'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                        'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                        'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                        'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                        'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']]\n",
    "\n",
    "# Adjust volumes for the hour\n",
    "Oldham_July_24hr_matrix_BA_2050 = Oldham_July_24hr_matrix_BA_2050[['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', \n",
    "                                                                      'Four_AM', 'Five_AM', 'Six_AM', 'Seven_AM', \n",
    "                                                                      'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "                                                                      'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', \n",
    "                                                                      'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "                                                                      'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']].multiply(\n",
    "    Oldham_July_24hr_matrix_BA_2050[\"BA_July_Volume\"], axis=\"index\")\n",
    "Oldham_July_24hr_matrix_BA_2050 = Oldham_July_24hr_matrix_BA_2050.fillna(0)\n",
    "\n",
    "Oldham_July_Hourly_Volume_BA_2050 = pd.concat([Oldham_July_volume_Only_BA_2050, Oldham_July_24hr_matrix_BA_2050], axis=1, join='inner')\n",
    "\n",
    "# Apply BPR calculation function for each hour for BA # Copy this section incase of error\n",
    "\n",
    "def calculate_bpr(hour, volume_data):\n",
    "    bp_col = f'{hour}_BP'\n",
    "    bpr_col = f'{hour}_BPR'\n",
    "    congested_speed_col = f'{hour}_BA_CONGESTED_SPEED'\n",
    "    new_vmt_col = f'{hour}_BA_New_VMT'\n",
    "    vht_col = f'{hour}_BA_VHT'\n",
    "    \n",
    "    volume_data[bp_col] = (volume_data[hour] / volume_data.BA_Peak_HR_Capacity) ** volume_data.B_exponent\n",
    "    volume_data[bpr_col] = 1 + (volume_data.A_coefficient * volume_data[bp_col])\n",
    "    volume_data[congested_speed_col] = (volume_data.BA_FFSPD / volume_data[bpr_col]) * volume_data.Speed_Adjustement_Factor\n",
    "    volume_data[new_vmt_col] = volume_data[hour] * volume_data.Length\n",
    "    volume_data[vht_col] = volume_data[new_vmt_col] / volume_data[congested_speed_col]\n",
    "\n",
    "# List of hours for which BPR needs to be calculated\n",
    "hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', 'Five_AM', \n",
    "         'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', 'Ten_AM', 'Eleven_AM', \n",
    "         'Twelve_PM', 'One_PM', 'Two_PM', 'Three_PM', 'Four_PM', 'Five_PM', \n",
    "         'Six_PM', 'Seven_PM', 'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "# Apply the function for each hour for BA\n",
    "for hour in hours:\n",
    "    calculate_bpr(hour, Oldham_July_Hourly_Volume_BA_2050)\n",
    "\n",
    "# Save results if needed Copy this section incase of error\n",
    "\n",
    "\n",
    "# Function to sum NEW VMT, VHT, and calculate mean of congested speed by direction (AB or BA)\n",
    "def sum_new_vmt_vht_and_speed_by_direction_2050(volume_data, direction='AB'):\n",
    "    # Create a new DataFrame to store the results\n",
    "    summary_df_2050 = pd.DataFrame(columns=['Hour', f'{direction}_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed'])\n",
    "\n",
    "    # List of hours for which NEW_VMT and VHT needs to be summed\n",
    "    hours = ['Twelve_AM', 'One_AM', 'Two_AM', 'Three_AM', 'Four_AM', \n",
    "             'Five_AM', 'Six_AM', 'Seven_AM', 'Eight_AM', 'Nine_AM', \n",
    "             'Ten_AM', 'Eleven_AM', 'Twelve_PM', 'One_PM', 'Two_PM', \n",
    "             'Three_PM', 'Four_PM', 'Five_PM', 'Six_PM', 'Seven_PM', \n",
    "             'Eight_PM', 'Nine_PM', 'Ten_PM', 'Eleven_PM']\n",
    "\n",
    "    # Iterate over each hour to sum NEW_VMT and VHT and calculate mean congested speed\n",
    "    for hour in hours:\n",
    "        new_vmt_col = f'{hour}_{direction}_New_VMT'  # Adjusted for AB or BA direction\n",
    "        vht_col = f'{hour}_{direction}_VHT'  # Adjusted for AB or BA direction\n",
    "        congested_speed_col = f'{hour}_{direction}_CONGESTED_SPEED'  # Adjusted for AB or BA direction\n",
    "        funcl_col = f'{direction}_FUNCL'  # Ensure this references AB_FUNCL or BA_FUNCL\n",
    "        \n",
    "        # Group by FUNCL and sum the NEW_VMT and VHT, and calculate mean congested speed\n",
    "        hourly_sum = volume_data.groupby(funcl_col).agg(\n",
    "            Total_NEW_VMT=(new_vmt_col, 'sum'),\n",
    "            Total_VHT=(vht_col, 'sum'),\n",
    "            Mean_Congested_Speed=(congested_speed_col, 'mean')  # Calculate mean directly\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add the hour label\n",
    "        hourly_sum['Hour'] = hour\n",
    "        \n",
    "        # Append to the summary DataFrame\n",
    "        summary_df_2050 = pd.concat([summary_df_2050, hourly_sum], ignore_index=True)\n",
    "\n",
    "    return summary_df_2050\n",
    "\n",
    "# Call the function for AB direction in Oldham\n",
    "funcl_summary_Oldham_ab_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Oldham_July_Hourly_Volume_2050, direction='AB')\n",
    "\n",
    "# Call the function for BA direction in Oldham\n",
    "funcl_summary_Oldham_ba_2050 = sum_new_vmt_vht_and_speed_by_direction_2050(Oldham_July_Hourly_Volume_BA_2050, direction='BA')\n",
    "\n",
    "# Aggregate the results for AB direction\n",
    "funcl_summary_Oldham_ab_2050 = funcl_summary_Oldham_ab_2050.loc[:, ['AB_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ab_2050 = funcl_summary_Oldham_ab_2050.groupby(['AB_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate the results for BA direction\n",
    "funcl_summary_Oldham_ba_2050 = funcl_summary_Oldham_ba_2050.loc[:, ['BA_FUNCL', 'Total_NEW_VMT', 'Total_VHT', 'Mean_Congested_Speed']]\n",
    "funcl_summary_Oldham_ba_2050 = funcl_summary_Oldham_ba_2050.groupby(['BA_FUNCL']).agg({\n",
    "    'Total_NEW_VMT': 'sum',\n",
    "    'Total_VHT': 'sum',\n",
    "    'Mean_Congested_Speed': 'mean'  # Ensure this is mean\n",
    "}).reset_index()\n",
    "\n",
    "# Combine the AB and BA summaries into a single DataFrame\n",
    "combined_summary_2050 = pd.merge(funcl_summary_Oldham_ab_2050, funcl_summary_Oldham_ba_2050, \n",
    "                             left_on='AB_FUNCL', right_on='BA_FUNCL', \n",
    "                             how='outer', suffixes=('_AB', '_BA'))\n",
    "\n",
    "# Calculate the total VMT and VHT across both directions\n",
    "combined_summary_2050['Total_NEW_VMT'] = combined_summary_2050['Total_NEW_VMT_AB'].fillna(0) + combined_summary_2050['Total_NEW_VMT_BA'].fillna(0)\n",
    "combined_summary_2050['Total_VHT'] = combined_summary_2050['Total_VHT_AB'].fillna(0) + combined_summary_2050['Total_VHT_BA'].fillna(0)\n",
    "\n",
    "# Calculate the mean congested speed across both directions\n",
    "mean_speed_ab_2050 = combined_summary_2050['Mean_Congested_Speed_AB'].fillna(0)\n",
    "mean_speed_ba_2050 = combined_summary_2050['Mean_Congested_Speed_BA'].fillna(0)\n",
    "\n",
    "# Calculate the overall mean speed by averaging both direction speeds\n",
    "combined_summary_2050['Mean_Congested_Speed'] = (mean_speed_ab_2050 + mean_speed_ba_2050) / 2\n",
    "\n",
    "# Prepare the final DataFrame for Oldham\n",
    "Oldham_VMT_2050 = pd.DataFrame({\n",
    "    'AB_FUNCL': combined_summary_2050['AB_FUNCL'],  # Including AB_FUNCL\n",
    "    'Total_NEW_VMT': combined_summary_2050['Total_NEW_VMT'],\n",
    "    'Total_VHT': combined_summary_2050['Total_VHT'],\n",
    "    'Mean_Congested_Speed': combined_summary_2050['Mean_Congested_Speed']\n",
    "})\n",
    "\n",
    "# Print the final summary for Oldham\n",
    "#print(Oldham_VMT)\n",
    "#Oldham_VMT\n",
    "\n",
    "# Assuming Oldham_VMT is defined elsewhere in your code\n",
    "Oldham_VMT_df_2050 = pd.DataFrame(Oldham_VMT_2050)\n",
    "\n",
    "# List of expected values\n",
    "expected_values = [\n",
    "    \"RURAL INTERSTATES\", \"RURAL FREEWAYS/X-WAYS\", \"RURAL PRINCIPAL ARTERIALS\",\n",
    "    \"RURAL MINOR ARTERIALS\", \"RURAL MAJOR COLLECTORS\", \"RURAL MINOR COLLECTORS\",\n",
    "    \"RURAL LOCAL ROADS\", \"RURAL CENTROID CONNECTORS\", \"RURAL RAMPS\", \n",
    "    \"URBAN INTERSTATES\", \"URBAN FREEWAYS/X-WAYS\", \"URBAN PRINCIPAL ARTERIALS\",\n",
    "    \"URBAN MINOR ARTERIALS\", \"URBAN MAJOR COLLECTORS\", \"URBAN MINOR COLLECTORS\",\n",
    "    \"URBAN LOCAL ROADS\", \"URBAN CENTROID CONNECTORS\", \"URBAN RAMPS\"\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the expected values\n",
    "expected_Oldham_VMT_df = pd.DataFrame({\"AB_FUNCL\": expected_values})\n",
    "\n",
    "# Merge the two DataFrames, filling missing values with 0\n",
    "final_Oldham_VMT_df_2050 = pd.merge(expected_Oldham_VMT_df, Oldham_VMT_df_2050, on=\"AB_FUNCL\", how=\"left\").fillna(0)\n",
    "\n",
    "# Optionally, convert numerical columns back to the correct types\n",
    "final_Oldham_VMT_df_2050[\"Total_NEW_VMT\"] = final_Oldham_VMT_df_2050[\"Total_NEW_VMT\"].astype(float)\n",
    "final_Oldham_VMT_df_2050[\"Total_VHT\"] = final_Oldham_VMT_df_2050[\"Total_VHT\"].astype(float)\n",
    "final_Oldham_VMT_df_2050[\"Mean_Congested_Speed\"] = final_Oldham_VMT_df_2050[\"Mean_Congested_Speed\"].astype(float)\n",
    "\n",
    "# Display the final DataFrame\n",
    "\n",
    "\n",
    "# Calculate the sum for each column\n",
    "\n",
    "# Display the DataFrame\n",
    "# Calculate total sums\n",
    "# Split DataFrame into rural and urban subsets\n",
    "# Split DataFrame into rural and urban subsets Extra!!!!!\n",
    "rural_df = final_Oldham_VMT_df_2050[final_Oldham_VMT_df_2050['AB_FUNCL'].str.contains('RURAL')]\n",
    "urban_df = final_Oldham_VMT_df_2050[final_Oldham_VMT_df_2050['AB_FUNCL'].str.contains('URBAN')]\n",
    "\n",
    "# Calculate total sums for rural and urban VMT and VHT\n",
    "total_rural_vmt = rural_df['Total_NEW_VMT'].sum()\n",
    "total_rural_vht = rural_df['Total_VHT'].sum()\n",
    "\n",
    "total_urban_vmt = urban_df['Total_NEW_VMT'].sum()\n",
    "total_urban_vht = urban_df['Total_VHT'].sum()\n",
    "\n",
    "# Calculate grand total VMT and VHT\n",
    "grand_total_vmt = total_rural_vmt + total_urban_vmt\n",
    "grand_total_vht = total_rural_vht + total_urban_vht\n",
    "\n",
    "# Create two new DataFrame rows for rural and urban totals\n",
    "rural_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL RURAL'],  # Label for rural total row\n",
    "    'Total_NEW_VMT': [total_rural_vmt],\n",
    "    'Total_VHT': [total_rural_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "urban_totals_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['TOTAL URBAN'],  # Label for urban total row\n",
    "    'Total_NEW_VMT': [total_urban_vmt],\n",
    "    'Total_VHT': [total_urban_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Create the grand total row\n",
    "grand_total_row = pd.DataFrame({\n",
    "    'AB_FUNCL': ['GRAND TOTAL'],  # Label for grand total row\n",
    "    'Total_NEW_VMT': [grand_total_vmt],\n",
    "    'Total_VHT': [grand_total_vht],\n",
    "    'Mean_Congested_Speed': [0],  # Set to 0 for total row\n",
    "    'Speed': [0]  # Set to 0 for total row\n",
    "})\n",
    "\n",
    "# Append the rural, urban, and grand total rows to the original DataFrame\n",
    "final_Oldham_VMT_df_2050 = pd.concat([final_Oldham_VMT_df_2050, rural_totals_row, urban_totals_row, grand_total_row], ignore_index=True)\n",
    "\n",
    "final_Oldham_VMT_df_2050['Speed'] = final_Oldham_VMT_df_2050['Total_NEW_VMT'] / final_Oldham_VMT_df_2050['Total_VHT']\n",
    "\n",
    "\n",
    "#5\n",
    "# Function to add ramps/centroid connectors to interstates/local roads\n",
    "def add_ramps_or_centroids(df, primary_label, secondary_label, freeway_col_name=None, ramp_col_name=None):\n",
    "    # Get the rows for primary (interstates/local roads) and secondary (ramps/centroid connectors)\n",
    "    primary_row = df[df['AB_FUNCL'] == primary_label].iloc[0]\n",
    "    secondary_row = df[df['AB_FUNCL'] == secondary_label].iloc[0]\n",
    "    \n",
    "    # Add VMT and VHT\n",
    "    new_vmt = primary_row['Total_NEW_VMT'] + secondary_row['Total_NEW_VMT']\n",
    "    new_vht = primary_row['Total_VHT'] + secondary_row['Total_VHT']\n",
    "\n",
    "    # Calculate the weighted mean of Mean_Congested_Speed (weighted by VMT)\n",
    "    weighted_mean_congested_speed = (\n",
    "        (primary_row['Mean_Congested_Speed'] * primary_row['Total_NEW_VMT'] + \n",
    "         secondary_row['Mean_Congested_Speed'] * secondary_row['Total_NEW_VMT']) \n",
    "        / new_vmt\n",
    "    )\n",
    "\n",
    "    # Update the primary (interstates/local roads) row with new values for VMT, VHT, and Mean Congested Speed\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_NEW_VMT'] = new_vmt\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Total_VHT'] = new_vht\n",
    "    df.loc[df['AB_FUNCL'] == primary_label, 'Mean_Congested_Speed'] = weighted_mean_congested_speed\n",
    "    \n",
    "    # Calculate percentage shares for VMT (if applicable)\n",
    "    if freeway_col_name and ramp_col_name:\n",
    "        freeway_share = (primary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        ramp_share = (secondary_row['Total_NEW_VMT'] / new_vmt) * 100\n",
    "        \n",
    "        df.loc[df['AB_FUNCL'] == primary_label, freeway_col_name] = freeway_share\n",
    "        df.loc[df['AB_FUNCL'] == primary_label, ramp_col_name] = ramp_share\n",
    "\n",
    "    # Remove the secondary (ramps/centroid connectors) row from the dataframe\n",
    "    df = df[df['AB_FUNCL'] != secondary_label]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example DataFrame (final_Oldham_VMT_df_2050) structure as provided previously\n",
    "\n",
    "# Add rural ramps to rural interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2050, \n",
    "    'RURAL INTERSTATES', \n",
    "    'RURAL RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add urban ramps to urban interstates and calculate VMT shares\n",
    "final_Oldham_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2050, \n",
    "    'URBAN INTERSTATES', \n",
    "    'URBAN RAMPS', \n",
    "    '% M6 Freeway', \n",
    "    '% M6 Ramp'\n",
    ")\n",
    "\n",
    "# Add rural centroid connectors to rural local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2050, \n",
    "    'RURAL LOCAL ROADS', \n",
    "    'RURAL CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "# Add urban centroid connectors to urban local roads (no M6 calculations)\n",
    "final_Oldham_VMT_df_2050 = add_ramps_or_centroids(\n",
    "    final_Oldham_VMT_df_2050, \n",
    "    'URBAN LOCAL ROADS', \n",
    "    'URBAN CENTROID CONNECTORS'\n",
    ")\n",
    "\n",
    "\n",
    "#final_Oldham_VMT_df_2050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73768360-7c08-4644-8045-7526a0d6cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_Oldham_VMT_df_2050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247bb389-f7d9-4431-97f8-854dc96cde1d",
   "metadata": {},
   "source": [
    "# Combine 2050 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "939f1264-2ae5-4838-ab30-c624009b9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Create a blank DataFrame with the same columns to use for spacing\n",
    "blank_lines = pd.DataFrame(np.nan, index=range(5), columns=Clark_VMT.columns)\n",
    "\n",
    "# Initialize a list to hold the DataFrames and their titles\n",
    "dataframes_with_titles = []\n",
    "\n",
    "# Append each DataFrame with its title and blank lines\n",
    "for county, df in zip(['Bullit', 'Clark', 'Floyd', 'Jefferson', 'Oldham'], \n",
    "                      [final_Bullit_VMT_df_2050, final_Clark_VMT_df_2050, final_Floyd_VMT_df_2050, final_Jefferson_VMT_df_2050, final_Oldham_VMT_df_2050]):\n",
    "    # Create a DataFrame for the title (bold)\n",
    "    title_df = pd.DataFrame(columns=df.columns)\n",
    "    title_df.loc[0] = df.columns  # Set the title row\n",
    "    # Append the county name with \"County\" as the title\n",
    "    dataframes_with_titles.append((f\"{county} County\", title_df, df, blank_lines))\n",
    "\n",
    "# Create a new Excel workbook\n",
    "output_file_path = '2050_Combined_VMT_Data.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = 'VMT_Data'\n",
    "\n",
    "# Add the dataframes and format them\n",
    "row_offset = 0\n",
    "for title, title_df, df, blanks in dataframes_with_titles:\n",
    "    # Add the file name with \"County\" as a subheading above each DataFrame\n",
    "    ws.cell(row=row_offset + 1, column=1, value=title).font = Font(bold=True)\n",
    "    row_offset += 1  # Move to the next row for the title\n",
    "\n",
    "    # Write the column titles (bold)\n",
    "    for c_idx, column_name in enumerate(title_df.columns, start=1):\n",
    "        ws.cell(row=row_offset + 1, column=c_idx, value=column_name).font = Font(bold=True)\n",
    "\n",
    "    # Write the DataFrame data\n",
    "    for r_idx, row in enumerate(df.values, start=row_offset + 2):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Adjust column widths based on the maximum length of data in each column\n",
    "    for c_idx in range(1, df.shape[1] + 1):\n",
    "        max_length = 0\n",
    "        # Check title length\n",
    "        max_length = max(max_length, len(title_df.columns[c_idx - 1]))  \n",
    "        # Check data length\n",
    "        for r_idx in range(df.shape[0]):\n",
    "            cell_value = str(df.iat[r_idx, c_idx - 1])  # Get cell value\n",
    "            max_length = max(max_length, len(cell_value))  # Update max length if necessary\n",
    "        ws.column_dimensions[ws.cell(row=1, column=c_idx).column_letter].width = max_length + 2  # Add some padding\n",
    "\n",
    "    # Update the row_offset for the next DataFrame\n",
    "    row_offset += df.shape[0] + 3  # 2 for the data and 1 for the title\n",
    "\n",
    "    # Add blank lines\n",
    "    for _ in range(5):\n",
    "        row_offset += 1  # Add blank lines between DataFrames\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09854f06-4dc3-41f8-a3aa-667ab387b7c2",
   "metadata": {},
   "source": [
    "# End "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1a2d2e2-0836-49e3-bc25-023d50a31d9e",
   "metadata": {},
   "source": [
    "# Combine all DataFrames into one\n",
    "combined_vmt = pd.concat([Clark_VMT, Floyd_VMT, Jefferson_VMT, Oldham_VMT, Bullit_VMT], ignore_index=True)\n",
    "\n",
    "# Write the combined DataFrame to an Excel file\n",
    "output_file_path = 'Combined_VMT_Data.xlsx'\n",
    "combined_vmt.to_excel(output_file_path, index=False, sheet_name='VMT_Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11b756-21b8-42a4-b661-f26ba2057163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b12fb368-6d34-45d0-b1c9-f6d2d40bb334",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('MovesPreprocessorModel2.pkl', 'wb') as file:\n",
    "    pickle.dump(MovesPreprocessorModel2, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdc053-27e4-4894-bd48-dca1b1983fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "98d6b7f8-1300-44a4-b9a4-76776ebf5c13",
   "metadata": {},
   "source": [
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Load the notebook\n",
    "notebook_file = 'MovesPreprocessorModel2.ipynb'  # Change this to your notebook's filename\n",
    "with open(notebook_file, 'r', encoding='utf-8') as f:\n",
    "    notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Convert to Python script\n",
    "python_exporter = PythonExporter()\n",
    "script, _ = python_exporter.from_notebook_node(notebook_content)\n",
    "\n",
    "# Save to .py file\n",
    "with open('MovesPreprocessorModel2.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc407f-10ff-463a-b33d-8ce2c81fca82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
